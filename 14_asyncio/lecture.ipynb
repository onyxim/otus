{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #14. AsyncIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [coroutines](#coroutines)\n",
    "2. [Yield from](#Yield-from)\n",
    "3. [Futures](#Futures)\n",
    "4. [Event loop](#Event-loop)\n",
    "5. [asyncio](#asyncio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infrastructure for coroutines appeared in *PEP 342 — Coroutines via Enhanced Generators*, implemented in Python 2.5 (2006): since then, the `yield` keyword can be used in an expression, and the `.send(value)` method was added to the generator API.\n",
    "Using `.send(...)`, the caller of the generator can post data that then becomes the value of the `yield` expression inside the generator function. \n",
    "\n",
    "This allows a generator to be used as a __coroutine__: a procedure that collaborates with the caller, yielding and receiving values from the caller.\n",
    "\n",
    "Or more general: __coroutines__ are computer-program components that generalize subroutines for non-preemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getgeneratorstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoException(Exception):\n",
    "    \"\"\"An exception type for the demonstration.\"\"\"\n",
    "\n",
    "\n",
    "def demo_exc_handling():\n",
    "    print('-> coroutine started')\n",
    "    while True:\n",
    "        try:\n",
    "            x = yield\n",
    "        except DemoException:\n",
    "            print('*** DemoException handled. Continuing...')\n",
    "        else:\n",
    "            print('-> coroutine received: {!r}'.format(x))\n",
    "    raise RuntimeError('This line should never run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prime coroutine before use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't send non-None value to a just-started generator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f2c614797933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexc_coro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo_exc_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexc_coro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't send non-None value to a just-started generator"
     ]
    }
   ],
   "source": [
    "exc_coro = demo_exc_handling()\n",
    "exc_coro.send(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial call `next(exc_coro)` is often described as “priming” the coroutine (i.e., advancing it to the first yield to make it ready for use as a live coroutine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> coroutine started\n",
      "-> coroutine received: 11\n",
      "-> coroutine received: 42\n",
      "GEN_CLOSED\n"
     ]
    }
   ],
   "source": [
    "exc_coro = demo_exc_handling()\n",
    "next(exc_coro) # or exc_coro.send(None)\n",
    "exc_coro.send(11)\n",
    "exc_coro.send(42)\n",
    "exc_coro.close()\n",
    "print(getgeneratorstate(exc_coro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> coroutine started\n",
      "-> coroutine received: 11\n",
      "-> coroutine received: 42\n",
      "*** DemoException handled. Continuing...\n",
      "GEN_SUSPENDED\n"
     ]
    }
   ],
   "source": [
    "exc_coro = demo_exc_handling()\n",
    "next(exc_coro)\n",
    "exc_coro.send(11)\n",
    "exc_coro.send(42)\n",
    "exc_coro.throw(DemoException)\n",
    "print(getgeneratorstate(exc_coro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s crucial to understand that the execution of the coroutine is suspended exactly at the `yield` keyword. In an assignment statement, the code to the right of the `=` is evaluated before the actual assignment happens. This means that in a line like `b = yield a`, the value of `b` will only be set when the coroutine is activated later by the client code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![coro](https://dl.dropbox.com/s/hr2i9ejpo2tly2v/%D0%A1%D0%BA%D1%80%D0%B8%D0%BD%D1%88%D0%BE%D1%82%202018-11-19%2017.15.35.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def coroutine(func):\n",
    "    \"\"\"Decorator: primes `func` by advancing to first `yield`\"\"\"\n",
    "    @wraps(func)\n",
    "    def primer(*args,**kwargs):\n",
    "        gen = func(*args,**kwargs)\n",
    "        next(gen)\n",
    "        return gen\n",
    "    return primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yield from` syntax automatically primes the coroutine called by it, making it incompatible with decorators such as `@coroutine`. The `asyncio.coroutine` decorator from the Python 3.4 standard library is designed to work with `yield from` so it does not prime the coroutine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@coroutine\n",
    "def averager():\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    average = None\n",
    "    while True:\n",
    "        term = yield average\n",
    "        total += term\n",
    "        count += 1\n",
    "        average = total/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN_SUSPENDED\n",
      "10.0\n",
      "20.0\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "coro_avg = averager() \n",
    "print(getgeneratorstate(coro_avg))\n",
    "print(coro_avg.send(10))\n",
    "print(coro_avg.send(30))\n",
    "print(coro_avg.send(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Result = namedtuple('Result', 'count average')\n",
    "\n",
    "@coroutine\n",
    "def averager():\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    average = None\n",
    "    while True:\n",
    "        term = yield\n",
    "        if term is None:\n",
    "            break\n",
    "        total += term\n",
    "        count += 1\n",
    "        average = total/count\n",
    "    return Result(count, average) # Before Python 3.3, it was a syntax error to return a value in a generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(count=3, average=15.5)\n"
     ]
    }
   ],
   "source": [
    "coro_avg = averager()\n",
    "coro_avg.send(10)\n",
    "coro_avg.send(30)\n",
    "coro_avg.send(6.5)\n",
    "try:\n",
    "    coro_avg.send(None)\n",
    "except StopIteration as exc:\n",
    "    result = exc.value\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This roundabout way of getting the return value from a coroutine makes more sense when we realize it was defined as part of *PEP 380*, and the `yield from` construct handles it automatically by catching `StopIteration` internally. This is analogous to the use of `StopIteration` in `for` loops: the exception is handled by the loop machinery in a way that is transparent to the user. In the case of `yield from`, the interpreter not only consumes the `StopIteration`, but its value attribute becomes the value of the `yield from` expression itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yield from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest evolutionary step for coroutines came with *PEP 380 - Syntax for Delegating to a Subgenerator*, implemented in Python 3.3 (2012). *PEP 380* made two syntax changes to generator functions, to make them more useful as coroutines:\n",
    "* A generator can now `return` a value; previously, providing a value to the return statement inside a generator raised a `SyntaxError`.\n",
    "* The `yield from` syntax enables complex generators to be refactored into smaller, nested generators while avoiding a lot of boilerplate code previously required for a generator to delegate to subgenerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 0, 1, 2]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chain(*iterables):\n",
    "    for it in iterables:\n",
    "        yield from it\n",
    "\n",
    "s = 'ABC'\n",
    "t = tuple(range(3))\n",
    "list(chain(s, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Dave\n",
      "Paula\n",
      "Thomas\n",
      "Lewis\n"
     ]
    }
   ],
   "source": [
    "# Example of flattening a nested sequence using subgenerators\n",
    "\n",
    "from collections import Iterable\n",
    "\n",
    "def flatten(items, ignore_types=(str, bytes)):\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, ignore_types):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "items = [1, 2, [3, 4, [5, 6], 7], 8]\n",
    "\n",
    "# Produces 1 2 3 4 5 6 7 8\n",
    "for x in flatten(items):\n",
    "    print(x)\n",
    "\n",
    "items = ['Dave', 'Paula', ['Thomas', 'Lewis']]\n",
    "for x in flatten(items):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar constructs in other languages are called `await`: when a generator `gen` calls `yield from subgen()`, the `subgen` takes over and will yield values to the caller of `gen`; the caller will in effect drive `subgen` directly. Meanwhile `gen` will be blocked, waiting until `subgen` terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main feature of `yield from` is to open a bidirectional channel from the outermost caller to the innermost subgenerator, so that values can be sent and yielded back and forth directly from them, and exceptions can be thrown all the way in without adding a lot of exception handling boilerplate code in the intermediate coroutines. This is what enables coroutine delegation in a way that was not possible before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![from](https://dl.dropbox.com/s/jfiuxh713g8o87s/%D0%A1%D0%BA%D1%80%D0%B8%D0%BD%D1%88%D0%BE%D1%82%202018-11-19%2017.16.02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9 boys  averaging 40.42kg\n",
      " 9 boys  averaging 1.39m\n",
      "10 girls averaging 42.04kg\n",
      "10 girls averaging 1.43m\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "data = {\n",
    "    'girls;kg':\n",
    "        [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5],\n",
    "    'girls;m':\n",
    "        [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43],\n",
    "    'boys;kg':\n",
    "        [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3],\n",
    "    'boys;m':\n",
    "        [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],\n",
    "}\n",
    "\n",
    "\n",
    "Result = namedtuple('Result', 'count average')\n",
    "\n",
    "\n",
    "# the subgenerator\n",
    "def averager():\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    average = None\n",
    "    while True:\n",
    "        term = yield\n",
    "        if term is None:\n",
    "            break\n",
    "        total += term\n",
    "        count += 1\n",
    "        average = total/count\n",
    "    return Result(count, average)\n",
    "\n",
    "\n",
    "# the delegating generator\n",
    "def grouper(results, key):\n",
    "    while True:\n",
    "        results[key] = yield from averager()\n",
    "\n",
    "        \n",
    "# the client code, a.k.a. the caller\n",
    "def main(data):\n",
    "    results = {}\n",
    "    for key, values in data.items():\n",
    "        group = grouper(results, key)\n",
    "        next(group)\n",
    "        for value in values:\n",
    "            group.send(value)\n",
    "        group.send(None)  # important!\n",
    "\n",
    "    # print(results)  # uncomment to debug\n",
    "    report(results)\n",
    "\n",
    "\n",
    "# output report\n",
    "def report(results):\n",
    "    for key, result in sorted(results.items()):\n",
    "        group, unit = key.split(';')\n",
    "        print('{:2} {:5} averaging {:.2f}{}'.format(\n",
    "              result.count, group, result.average, unit))\n",
    "\n",
    "        \n",
    "main(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every yield from chain must be driven by a client that calls `next(...)` or `.send(...)` on the outermost delegating generator. This call may be implicit, such as a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PEP 380* draft used to say about `yield from`:\n",
    "“When the iterator is another generator, the effect is the same as if the body of the sub‐generator were inlined at the point of the `yield from` expression. Furthermore, the subgenerator is allowed to execute a return statement with a value, and that value becomes the value of the `yield from` expression.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approved version of *PEP 380* explains the behavior of yield from in six points in the Proposal section.:\n",
    "* Any values that the subgenerator yields are passed directly to the caller of the delegating generator (i.e., the client code).\n",
    "* Any values sent to the delegating generator using `send()` are passed directly to the subgenerator. If the sent value is `None`, the subgenerator’s `__next__()` method is called. If the sent value is not `None`, the subgenerator’s `send()` method is called. If the call raises `StopIteration`, the delegating generator is resumed. Any other exception is propagated to the delegating generator.\n",
    "* `return expr` in a generator (or subgenerator) causes `StopIteration(expr)` to be raised upon exit from the generator.\n",
    "* The value of the `yield from` expression is the first argument to the `StopIteration` exception raised by the subgenerator when it terminates.\n",
    "* Exceptions other than `GeneratorExit` thrown into the delegating generator are passed to the `throw()` method of the subgenerator. If the call raises `StopIteration`, the delegating generator is resumed. Any other exception is propagated to the delegating generator.\n",
    "* If a `GeneratorExit` exception is thrown into the delegating generator, or the `close()` method of the delegating generator is called, then the close() method of the subgenerator is called if it has one. If this call results in an exception, it is propagated to the delegating generator. Otherwise, `GeneratorExit` is raised in the delegating generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RESULT = yield from EXPR` is semantically equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_i = iter(EXPR)\n",
    "try:\n",
    "    _y = next(_i)\n",
    "except StopIteration as _e:\n",
    "    _r = _e.value\n",
    "else:\n",
    "    while 1:\n",
    "        try:\n",
    "            _s = yield _y\n",
    "        except GeneratorExit as _e:\n",
    "            try:\n",
    "                _m = _i.close\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            else:\n",
    "                _m()\n",
    "            raise _e\n",
    "        except BaseException as _e:\n",
    "            _x = sys.exc_info()\n",
    "            try:\n",
    "                _m = _i.throw\n",
    "            except AttributeError:\n",
    "                raise _e\n",
    "            else:\n",
    "                try:\n",
    "                    _y = _m(*_x)\n",
    "                except StopIteration as _e:\n",
    "                    _r = _e.value\n",
    "                    break\n",
    "        else:\n",
    "            try:\n",
    "                if _s is None:\n",
    "                    _y = next(_i)\n",
    "                else:\n",
    "                    _y = _i.send(_s)\n",
    "            except StopIteration as _e:\n",
    "                _r = _e.value\n",
    "                break\n",
    "RESULT = _r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_key(row):\n",
    "    return row[\"!Sample_title\"], row[\"!Sample_geo_accession\"]\n",
    "\n",
    "def load_csv(filename):\n",
    "    \"\"\"Put csv data into a dict that maps title/geo to the complete row.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(filename) as f:\n",
    "        for row in csv.DictReader(f, delimiter=\",\"):\n",
    "            key = get_key(row)\n",
    "            assert key not in d\n",
    "            d[key] = row\n",
    "    return d\n",
    "\n",
    "def diffs(old, new):\n",
    "    yield from added_or_removed(\"ADDED\", new.keys() - old.keys(), new)\n",
    "    yield from added_or_removed(\"REMOVED\", old.keys() - new.keys(), old)\n",
    "    yield from changed(old, new)\n",
    "\n",
    "def compare_row(key, old, new):\n",
    "    i = -1\n",
    "    for i, line in enumerate(diffs(old, new)):\n",
    "        if not i:\n",
    "            print(\"/\".join(key))\n",
    "        print(\"    \" + line)\n",
    "    if i >= 0:\n",
    "        print()\n",
    "\n",
    "def added_or_removed(state, keys, d):\n",
    "    items = sorted((key, d[key]) for key in keys)\n",
    "    for key, value in items:\n",
    "        yield \"{:10}: {:30} | {:30}\".format(state, key, value)\n",
    "\n",
    "def changed(old, new):\n",
    "    common_columns = old.keys() & new.keys()\n",
    "    for column in sorted(common_columns):\n",
    "        oldvalue = old[column]\n",
    "        newvalue = new[column]\n",
    "        if oldvalue != newvalue:\n",
    "            yield \"{:10}: {:30} | {:30} | {:30}\".format(\n",
    "            \"CHANGED\",\n",
    "            column, \n",
    "            oldvalue.ljust(30),\n",
    "            newvalue.ljust(30))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    oldcsv = load_csv(\"2.csv\")\n",
    "    newcsv = load_csv(\"1.csv\")\n",
    "    # title/geo pairs that occur in both files:\n",
    "    common = oldcsv.keys() & newcsv.keys() \n",
    "    for key in sorted(common):\n",
    "        compare_row(key, oldcsv[key], newcsv[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding: '<foo>'\n",
      "Feeding: 'This'\n",
      "Feeding: 'is'\n",
      "Feeding: 'a'\n",
      "Feeding: '<b>'\n",
      "Feeding: 'foo'\n",
      "Feeding: 'file'\n",
      "Feeding: '</b>'\n",
      "Feeding: 'you'\n",
      "Feeding: 'know.'\n",
      "Feeding: '</foo>'\n",
      "[('foo', ['This', 'is', 'a', ('b', ['foo', 'file']), 'you', 'know.'])]\n"
     ]
    }
   ],
   "source": [
    "import re, sys\n",
    "pat = re.compile(r\"(\\S+)|(<[^>]*>)\")\n",
    "\n",
    "text = \"<foo> This is a <b> foo file </b> you know. </foo>\"\n",
    "\n",
    "def run():\n",
    "    parser = parse_items()\n",
    "    next(parser)\n",
    "    try:\n",
    "        for m in pat.finditer(text):\n",
    "            token = m.group(0)\n",
    "            print(\"Feeding:\", repr(token))\n",
    "            parser.send(token)\n",
    "        parser.send(None) # to signal EOF\n",
    "    except StopIteration as e:\n",
    "        tree = e.value\n",
    "        print(tree)\n",
    "\n",
    "def parse_elem(opening_tag):\n",
    "    name = opening_tag[1:-1]\n",
    "    closing_tag = \"</%s>\" % name\n",
    "    items = yield from parse_items(closing_tag)\n",
    "    return (name, items)\n",
    "\n",
    "def parse_items(closing_tag = None):\n",
    "    elems = []\n",
    "    while 1:\n",
    "        token = yield\n",
    "        if not token:\n",
    "            break # EOF\n",
    "        if is_opening_tag(token):\n",
    "            e = yield from parse_elem(token)\n",
    "            elems.append(e)\n",
    "        elif token == closing_tag:\n",
    "            break\n",
    "        else:\n",
    "            elems.append(token)\n",
    "    return elems\n",
    "\n",
    "def is_opening_tag(token):\n",
    "    return token.startswith(\"<\") and not token.startswith(\"</\")\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inline future formulation with exception handling added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: 5\n",
      "\n",
      "Failed: TypeError(\"unsupported operand type(s) for +: 'int' and 'str'\",)\n"
     ]
    }
   ],
   "source": [
    "class Task:\n",
    "    def __init__(self, gen):\n",
    "        self._gen = gen\n",
    "\n",
    "    def step(self, value=None, exc=None):\n",
    "        try:\n",
    "            if exc:\n",
    "                fut = self._gen.throw(exc)\n",
    "            else:\n",
    "                fut = self._gen.send(value)\n",
    "            fut.add_done_callback(self._wakeup)\n",
    "        except StopIteration as exc:\n",
    "            pass\n",
    "\n",
    "    def _wakeup(self, fut):\n",
    "        try:\n",
    "            result = fut.result()\n",
    "            self.step(result, None)\n",
    "        except Exception as exc:\n",
    "            self.step(None, exc)\n",
    "\n",
    "# Example\n",
    "if __name__ == '__main__':\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    import time\n",
    "\n",
    "    pool = ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "    def func(x, y):\n",
    "        time.sleep(1)\n",
    "        return x + y\n",
    "\n",
    "    def do_func(x, y):\n",
    "        try:\n",
    "            result = yield pool.submit(func, x, y)\n",
    "            print('Got:', result, end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print('Failed:', repr(e))\n",
    "\n",
    "    t = Task(do_func(2,3))\n",
    "    t.step()\n",
    "\n",
    "    t = Task(do_func(2, 'Hello'))\n",
    "    t.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators in Python 2.2 let the execution of code be paused. Once the ability to send values back into the paused generators were introduced in Python 2.5, the concept of coroutines in Python became possible. And the addition of `yield from` in Python 3.3 made it easier to refactor generators as well as chain them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Python 3.4, there are two classes named Future in the standard library: `concurrent.futures.Future` and `asyncio.Future`. They serve the same purpose: an instance of either `Future` class represents a deferred computation that may or may not have completed. This is similar to the Deferred class in Twisted, the Future class in Tornado, and Promise objects in various JavaScript libraries.\n",
    "\n",
    "`asyncio.Future(*, loop=None)` is almost compatible with `concurrent.futures.Future`. Differences:\n",
    "* `result()` and `exception()` do not take a `timeout` argument and raise an exception when the future isn’t done yet.\n",
    "* Callbacks registered with `add_done_callback()` are always called via the event loop’s `call_soon_threadsafe()`.\n",
    "* This class is not compatible with the `wait()` and `as_completed()` functions in the `concurrent.futures package`.\n",
    "* This class is not thread safe.\n",
    "\n",
    "Futures encapsulate pending operations so that they can be put in queues, their state of completion can be queried, and their results (or exceptions) can be retrieved when available.\n",
    "\n",
    "An important thing to know about futures in general is that you should not create them: they are meant to be instantiated exclusively by the concurrency framework, be it `concurrent.futures` or `asyncio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "POP20_CC = ('CN IN US ID BR PK NG BD RU JP '\n",
    "            'MX PH VN ET EG DE IR TR CD FR').split()\n",
    "\n",
    "BASE_URL = 'http://flupy.org/data/flags'\n",
    "\n",
    "DEST_DIR = '/Users/s.stupnikov/Coding/otus/otus-python-0717/lection16/downloads/'\n",
    "\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "\n",
    "\n",
    "def save_flag(img, filename):\n",
    "    path = os.path.join(DEST_DIR, filename)\n",
    "    with open(path, 'wb') as fp:\n",
    "        fp.write(img)\n",
    "\n",
    "\n",
    "def get_flag(cc):\n",
    "    url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())\n",
    "    resp = requests.get(url)\n",
    "    return resp.content\n",
    "\n",
    "\n",
    "def show(text):\n",
    "    print(text, end=' ')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "def download_one(cc):\n",
    "    image = get_flag(cc)\n",
    "    show(cc)\n",
    "    save_flag(image, cc.lower() + '.gif')\n",
    "    return cc\n",
    "\n",
    "def main(download_many):\n",
    "    t0 = time.time()\n",
    "    count = download_many(POP20_CC)\n",
    "    elapsed = time.time() - t0\n",
    "    msg = '\\n{} flags downloaded in {:.2f}s'\n",
    "    print(msg.format(count, elapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main features of the `concurrent.futures` package are the `ThreadPoolExecutor` and `ProcessPoolExecutor` classes, which implement an interface that allows you to submit callables for execution in different threads or processes, respectively. The classes manage an internal pool of worker threads or processes, and a queue of tasks to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VNPKNGEGPHJPDERUTRFRBDINIDCNBR               ETMXUS   IR CD \n",
      "20 flags downloaded in 3.19s\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "\n",
    "def download_one(cc):\n",
    "    image = get_flag(cc)\n",
    "    show(cc)\n",
    "    save_flag(image, cc.lower() + '.gif')\n",
    "    return cc\n",
    "\n",
    "\n",
    "def download_many(cc_list):\n",
    "    workers = min(MAX_WORKERS, len(cc_list))\n",
    "    with futures.ThreadPoolExecutor(workers) as executor:\n",
    "        res = executor.map(download_one, sorted(cc_list))\n",
    "\n",
    "    return len(list(res))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(download_many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a practical look at futures, we can rewrite previous example to use the `concurrent.futures.as_completed` function, which takes an iterable of futures and returns an iterator that yields futures as they are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled for BR: <Future at 0x10bb55b70 state=running>\n",
      "Scheduled for CN: <Future at 0x10c402f98 state=running>\n",
      "Scheduled for ID: <Future at 0x10c412860 state=running>\n",
      "Scheduled for IN: <Future at 0x10c4081d0 state=pending>\n",
      "Scheduled for US: <Future at 0x10c412dd8 state=pending>\n",
      "ID BRCN  <Future at 0x10c412860 state=finished returned str> result: 'ID'\n",
      "<Future at 0x10bb55b70 state=finished returned str> result: 'BR'\n",
      "<Future at 0x10c402f98 state=finished returned str> result: 'CN'\n",
      "IN <Future at 0x10c4081d0 state=finished returned str> result: 'IN'\n",
      "US <Future at 0x10c412dd8 state=finished returned str> result: 'US'\n",
      "\n",
      "5 flags downloaded in 0.48s\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_many(cc_list):\n",
    "    cc_list = cc_list[:5]\n",
    "    with futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        to_do = []\n",
    "        for cc in sorted(cc_list):\n",
    "            future = executor.submit(download_one, cc)\n",
    "            to_do.append(future)\n",
    "            msg = 'Scheduled for {}: {}'\n",
    "            print(msg.format(cc, future))\n",
    "\n",
    "        results = []\n",
    "        for future in futures.as_completed(to_do):\n",
    "            res = future.result()\n",
    "            msg = '{} result: {!r}'\n",
    "            print(msg.format(future, res))\n",
    "            results.append(res)\n",
    "\n",
    "    return len(results)\n",
    "\n",
    "\n",
    "def main(download_many):\n",
    "    t0 = time.time()\n",
    "    count = download_many(POP20_CC)\n",
    "    elapsed = time.time() - t0\n",
    "    msg = '\\n{} flags downloaded in {:.2f}s'\n",
    "    print(msg.format(count, elapsed))\n",
    "\n",
    "    \n",
    "main(download_many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the SHA-256 hash of a dozen 1 MB byte arrays with the standard library `hashlib` package, which uses the OpenSSL library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "076b1d856f88d49e6f187f4d67cdbc40c7dc06cbf32fa2eae32b169effb15ab4\n",
      "b4b0e6516b13f14d24789a614bb21ae3137915bc723d45bc5297d7d5aecc07f4\n",
      "dd5b03701d1e56575dfea31f076958b28a75b82a58ad131eb2b194a19066f0e0\n",
      "51bfae8ace8f85b6e4ae8fc04299b021bcbf2960f62c87c5941d7d84e74718d4\n",
      "ce2ed28a43b17b2aecebb6e85a83dee4dd13327ecfbcb12f92138609bcd9e5a0\n",
      "9044b22c14dd3cadc9a4ddb57a2efad0df6a929b8b35d6eebb0427c6709a74d6\n",
      "4c336055973ca4546002631235b1a37ff29a0a04187c9d3e634c9a50276d661a\n",
      "e3facaad84a053e55d21b54de35aae2aaa369467b68c2cce5a259313bf77b951\n",
      "37dd5c12ee617dc53c46c00cd28fc99821aa6abe47c04a7e7eb82d2008095722\n",
      "bf696d8da2790b44dfee6e4ac7f5c2506c713a91038df7c95816dca0155264c8\n",
      "5c74488e43e361fd46267da38c23674a29130f2a4faa9f8f6d95dc2ba00acee0\n",
      "ffc79ab350262b99e07a11219305ae7fe4c9e24f521fad67f7aec5f10aae21ce\n",
      "4 workers, elapsed time: 14.63s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "from concurrent import futures\n",
    "from random import randrange\n",
    "\n",
    "JOBS = 12\n",
    "SIZE = 2**20\n",
    "STATUS = '{} workers, elapsed time: {:.2f}s'\n",
    "\n",
    "\n",
    "def sha(size):\n",
    "    data = bytearray(randrange(256) for i in range(size))\n",
    "    algo = hashlib.new('sha256')\n",
    "    algo.update(data)\n",
    "    return algo.hexdigest()\n",
    "\n",
    "\n",
    "def main(workers=None):\n",
    "    if workers:\n",
    "        workers = int(workers)\n",
    "    t0 = time.time()\n",
    "\n",
    "    with futures.ProcessPoolExecutor(workers) as executor:\n",
    "        actual_workers = executor._max_workers\n",
    "        to_do = (executor.submit(sha, SIZE) for i in range(JOBS))\n",
    "        for future in futures.as_completed(to_do):\n",
    "            res = future.result()\n",
    "            print(res)\n",
    "\n",
    "    print(STATUS.format(actual_workers, time.time() - t0))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) == 2:\n",
    "        workers = int(sys.argv[1])\n",
    "    else:\n",
    "        workers = None\n",
    "    main(workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python's case, `asyncio` was added to the standard library to provide an event loop. There's a focus on networking in `asyncio` which in the case of the event loop is to make the \"when A happens\" to be when I/O from a socket is ready for reading and/or writing (via the `selectors` module). Other than GUIs and I/O, event loops are also often used for executing code in another thread or subprocess and have the event loop act as the scheduler (i.e., cooperative multitasking). If you happen to understand Python's GIL, event loops are useful in cases where releasing the GIL is possible and useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandler:\n",
    "    def fileno(self):\n",
    "        'Return the associated file descriptor'\n",
    "        raise NotImplemented('must implement')\n",
    "\n",
    "    def wants_to_receive(self):\n",
    "        'Return True if receiving is allowed'\n",
    "        return False\n",
    "\n",
    "    def handle_receive(self):\n",
    "        'Perform the receive operation'\n",
    "        pass\n",
    "\n",
    "    def wants_to_send(self):\n",
    "        'Return True if sending is requested' \n",
    "        return False\n",
    "\n",
    "    def handle_send(self):\n",
    "        'Send outgoing data'\n",
    "        pass\n",
    "\n",
    "import select\n",
    "\n",
    "def event_loop(handlers):\n",
    "    while True:\n",
    "        wants_recv = [h for h in handlers if h.wants_to_receive()]\n",
    "        wants_send = [h for h in handlers if h.wants_to_send()]\n",
    "        can_recv, can_send, _ = select.select(wants_recv, wants_send, [])\n",
    "        for h in can_recv:\n",
    "            h.handle_receive()\n",
    "        for h in can_send:\n",
    "            h.handle_send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "from eventhandler import EventHandler, event_loop\n",
    "\n",
    "class UDPServer(EventHandler):\n",
    "    def __init__(self, address):\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.bind(address)\n",
    "\n",
    "    def fileno(self):\n",
    "        return self.sock.fileno()\n",
    "\n",
    "    def wants_to_receive(self):\n",
    "        return True\n",
    "\n",
    "class UDPTimeServer(UDPServer):\n",
    "    def handle_receive(self):\n",
    "        msg, addr = self.sock.recvfrom(1)\n",
    "        self.sock.sendto(time.ctime().encode('ascii'), addr)\n",
    "\n",
    "class UDPEchoServer(UDPServer):\n",
    "    def handle_receive(self):\n",
    "        msg, addr = self.sock.recvfrom(8192)\n",
    "        self.sock.sendto(msg, addr)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    handlers = [ UDPTimeServer(('',14000)), UDPEchoServer(('',15000))  ]\n",
    "    event_loop(handlers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`asyncio`, a package that implements concurrency with coroutines driven by an event loop. It’s one of the largest and most ambitious libraries ever added to Python. Guido van Rossum developed asyncio outside of the Python repository and gave the project a code name of “Tulip”—so you’ll see references to that flower when researching this topic online. For example, the main discussion group is still called python-tulip.\n",
    "Tulip was renamed to asyncio when it was added to the standard library in Python 3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Thread(Thread-5, initial)>\n",
      "| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking          Answer: 42\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "def spin(msg, done):\n",
    "    write, flush = sys.stdout.write, sys.stdout.flush\n",
    "    for char in itertools.cycle('|/-\\\\'):\n",
    "        status = char + ' ' + msg\n",
    "        write(status)\n",
    "        flush()\n",
    "        write('\\x08' * len(status))\n",
    "        if done.wait(.1):\n",
    "            break\n",
    "    write(' ' * len(status) + '\\x08' * len(status))\n",
    "\n",
    "\n",
    "def slow_function():\n",
    "    # pretend waiting a long time for I/O\n",
    "    time.sleep(3)\n",
    "    return 42\n",
    "\n",
    "\n",
    "def supervisor():\n",
    "    done = threading.Event()\n",
    "    spinner = threading.Thread(target=spin,\n",
    "                               args=('thinking!', done))\n",
    "    print('spinner object:', spinner)\n",
    "    spinner.start()\n",
    "    result = slow_function()\n",
    "    done.set()\n",
    "    spinner.join()\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    result = supervisor()\n",
    "    print('Answer:', result)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loop](https://cdn-images-1.medium.com/max/1600/0*s1GH0YO9ZNdEEDxo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AsyncIO spinner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Task pending coro=<spin() running at <ipython-input-1-3971047b2aec>:6>>\n",
      "| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking          Answer: 42\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def spin(msg):\n",
    "    write, flush = sys.stdout.write, sys.stdout.flush\n",
    "    for char in itertools.cycle('|/-\\\\'):\n",
    "        status = char + ' ' + msg\n",
    "        write(status)\n",
    "        flush()\n",
    "        write('\\x08' * len(status))\n",
    "        try:\n",
    "            yield from asyncio.sleep(.1)\n",
    "        except asyncio.CancelledError:\n",
    "            break\n",
    "    write(' ' * len(status) + '\\x08' * len(status))\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def slow_function():\n",
    "    # pretend waiting a long time for I/O\n",
    "    yield from asyncio.sleep(3)\n",
    "    return 42\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def supervisor():\n",
    "    spinner = asyncio.ensure_future(spin('thinking!'))\n",
    "    print('spinner object:', spinner)\n",
    "    result = yield from slow_function()\n",
    "    spinner.cancel()\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    result = loop.run_until_complete(supervisor())\n",
    "    loop.close()\n",
    "    print('Answer:', result)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spinner object: <Task pending coro=<spin() running at <ipython-input-1-4b6a3a1d2b81>:6>>\n",
      "| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking/ thinking- thinking\\ thinking| thinking          Answer: 42\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "\n",
    "async def spin(msg):\n",
    "    write, flush = sys.stdout.write, sys.stdout.flush\n",
    "    for char in itertools.cycle('|/-\\\\'):\n",
    "        status = char + ' ' + msg\n",
    "        write(status)\n",
    "        flush()\n",
    "        write('\\x08' * len(status))\n",
    "        try:\n",
    "            await asyncio.sleep(.1)\n",
    "        except asyncio.CancelledError:\n",
    "            break\n",
    "    write(' ' * len(status) + '\\x08' * len(status))\n",
    "\n",
    "\n",
    "async def slow_function():\n",
    "    # pretend waiting a long time for I/O\n",
    "    await asyncio.sleep(3)\n",
    "    return 42\n",
    "\n",
    "\n",
    "async def supervisor():\n",
    "    spinner = asyncio.ensure_future(spin('thinking!'))\n",
    "    print('spinner object:', spinner)\n",
    "    result = await slow_function()\n",
    "    spinner.cancel()\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    result = loop.run_until_complete(supervisor())\n",
    "    loop.close()\n",
    "    print('Answer:', result)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are functional differences between native coroutines using `async def` syntax and generator-based coroutines using the `asyncio.coroutine` decorator\n",
    "* Native coroutine objects do not implement \\__iter\\__ and  \\__next\\__ methods. Therefore, they cannot be iterated over or passed to iter(), list(), tuple() and other built-ins. They also cannot be used in a `for..in` loop. An attempt to use \\__iter\\__ or \\__next\\__ on a native coroutine object will result in a `TypeError` .\n",
    "* Plain generators cannot `yield from` native coroutines: doing so will result in a `TypeError` .\n",
    "* generator-based coroutines can `yield from` native coroutine objects.\n",
    "* `inspect.isgenerator()` and `inspect.isgeneratorfunction()` return False for native coroutine objects and native coroutine functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@asyncio.coroutine\n",
    "def py34_coro():\n",
    "    yield from stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def py35_coro():\n",
    "    await stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3           0 LOAD_GLOBAL              0 (stuff)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 GET_YIELD_FROM_ITER\n",
      "              6 LOAD_CONST               0 (None)\n",
      "              8 YIELD_FROM\n",
      "             10 POP_TOP\n",
      "             12 LOAD_CONST               0 (None)\n",
      "             14 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import dis\n",
    "dis.dis(py34_coro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of GET_YIELD_FROM_ITER, it simply checks if its argument is a generator or coroutine, otherwise it calls iter() on its argument "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2           0 LOAD_GLOBAL              0 (stuff)\n",
      "              2 CALL_FUNCTION            0\n",
      "              4 GET_AWAITABLE\n",
      "              6 LOAD_CONST               0 (None)\n",
      "              8 YIELD_FROM\n",
      "             10 POP_TOP\n",
      "             12 LOAD_CONST               0 (None)\n",
      "             14 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "dis.dis(py35_coro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But GET_AWAITABLE does something different. While the bytecode will accept a coroutine just like GET_YIELD_FROM_ITER, it will not accept a generator if has not been flagged as a coroutine. Beyond just coroutines, though, the bytecode will accepted an awaitable object. This makes yield from expressions and await expressions both accept coroutines while differing on whether they accept plain generators or awaitable objects, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An awaitable object is either a coroutine or an object that defines `__await__()` -- technically `collections.abc.Awaitable` -- which returns an iterator that is not a coroutine. An await expression is basically yield from but with restrictions of only working with awaitable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isawaitable(object):\n",
    "    \"\"\"Return true if object can be passed to an ``await`` expression.\"\"\"\n",
    "    return (isinstance(object, types.CoroutineType) or\n",
    "            isinstance(object, types.GeneratorType) and\n",
    "                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or\n",
    "            isinstance(object, collections.abc.Awaitable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async flag downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN RU FR BD ID PK PH VN DE EG CD CN BR ET US MX NG JP TR IR \n",
      "20 flags downloaded in 0.79s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "async def get_flag(cc):\n",
    "    url = '{}/{cc}/{cc}.gif'.format(BASE_URL, cc=cc.lower())\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        resp = await session.request('GET', url)\n",
    "        image = await resp.read()\n",
    "        return image\n",
    "\n",
    "\n",
    "async def download_one(cc):\n",
    "    image = await get_flag(cc)\n",
    "    show(cc)\n",
    "    save_flag(image, cc.lower() + '.gif')\n",
    "    return cc\n",
    "\n",
    "\n",
    "def download_many(cc_list):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    to_do = [download_one(cc) for cc in sorted(cc_list)]\n",
    "    wait_coro = asyncio.wait(to_do)\n",
    "    res, _ = loop.run_until_complete(wait_coro)\n",
    "    loop.close()\n",
    "\n",
    "    return len(res)\n",
    "\n",
    "\n",
    "def main(download_many):\n",
    "    t0 = time.time()\n",
    "    count = download_many(POP20_CC)\n",
    "    elapsed = time.time() - t0\n",
    "    msg = '\\n{} flags downloaded in {:.2f}s'\n",
    "    print(msg.format(count, elapsed))\n",
    "\n",
    "\n",
    "main(download_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import collections\n",
    "import contextlib\n",
    "\n",
    "import aiohttp\n",
    "from aiohttp import web\n",
    "import tqdm\n",
    "\n",
    "from flags2_common import main, HTTPStatus, Result, save_flag\n",
    "\n",
    "# default set low to avoid errors from remote site, such as\n",
    "# 503 - Service Temporarily Unavailable\n",
    "DEFAULT_CONCUR_REQ = 5\n",
    "MAX_CONCUR_REQ = 1000\n",
    "\n",
    "\n",
    "class FetchError(Exception):\n",
    "    def __init__(self, country_code):\n",
    "        self.country_code = country_code\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def get_flag(base_url, cc):\n",
    "    url = '{}/{cc}/{cc}.gif'.format(base_url, cc=cc.lower())\n",
    "    resp = yield from aiohttp.request('GET', url)\n",
    "    with contextlib.closing(resp):\n",
    "        if resp.status == 200:\n",
    "            image = yield from resp.read()\n",
    "            return image\n",
    "        elif resp.status == 404:\n",
    "            raise web.HTTPNotFound()\n",
    "        else:\n",
    "            raise aiohttp.HttpProcessingError(\n",
    "                code=resp.status, message=resp.reason,\n",
    "                headers=resp.headers)\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def download_one(cc, base_url, semaphore, verbose):\n",
    "    try:\n",
    "        with (yield from semaphore):\n",
    "            image = yield from get_flag(base_url, cc)\n",
    "    except web.HTTPNotFound:\n",
    "        status = HTTPStatus.not_found\n",
    "        msg = 'not found'\n",
    "    except Exception as exc:\n",
    "        raise FetchError(cc) from exc\n",
    "    else:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_in_executor(None, save_flag, image, cc.lower() + '.gif')\n",
    "        status = HTTPStatus.ok\n",
    "        msg = 'OK'\n",
    "\n",
    "    if verbose and msg:\n",
    "        print(cc, msg)\n",
    "\n",
    "    return Result(status, cc)\n",
    "\n",
    "@asyncio.coroutine\n",
    "def downloader_coro(cc_list, base_url, verbose, concur_req):\n",
    "    counter = collections.Counter()\n",
    "    semaphore = asyncio.Semaphore(concur_req)\n",
    "    to_do = [download_one(cc, base_url, semaphore, verbose)\n",
    "             for cc in sorted(cc_list)]\n",
    "\n",
    "    to_do_iter = asyncio.as_completed(to_do)\n",
    "    if not verbose:\n",
    "        to_do_iter = tqdm.tqdm(to_do_iter, total=len(cc_list))\n",
    "    for future in to_do_iter:\n",
    "        try:\n",
    "            res = yield from future\n",
    "        except FetchError as exc:\n",
    "            country_code = exc.country_code\n",
    "            try:\n",
    "                error_msg = exc.__cause__.args[0]\n",
    "            except IndexError:\n",
    "                error_msg = exc.__cause__.__class__.__name__\n",
    "            if verbose and error_msg:\n",
    "                msg = '*** Error for {}: {}'\n",
    "                print(msg.format(country_code, error_msg))\n",
    "            status = HTTPStatus.error\n",
    "        else:\n",
    "            status = res.status\n",
    "\n",
    "        counter[status] += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def download_many(cc_list, base_url, verbose, concur_req):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    coro = downloader_coro(cc_list, base_url, verbose, concur_req)\n",
    "    counts = loop.run_until_complete(coro)\n",
    "    loop.close()\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCP Echo server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def handle_echo(reader, writer):\n",
    "    data = await reader.read(100)\n",
    "    message = data.decode()\n",
    "    addr = writer.get_extra_info('peername')\n",
    "    print(\"Received %r from %r\" % (message, addr))\n",
    "\n",
    "    print(\"Send: %r\" % message)\n",
    "    writer.write(data)\n",
    "    await writer.drain()\n",
    "\n",
    "    print(\"Close the client socket\")\n",
    "    writer.close()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "coro = asyncio.start_server(handle_echo, '127.0.0.1', 8888, loop=loop)\n",
    "server = loop.run_until_complete(coro)\n",
    "\n",
    "# Serve requests until Ctrl+C is pressed\n",
    "print('Serving on {}'.format(server.sockets[0].getsockname()))\n",
    "try:\n",
    "    loop.run_forever()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# Close the server\n",
    "server.close()\n",
    "loop.run_until_complete(server.wait_closed())\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCP Echo client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def tcp_echo_client(message, loop):\n",
    "    reader, writer = await asyncio.open_connection('127.0.0.1', 8888,\n",
    "                                                   loop=loop)\n",
    "\n",
    "    print('Send: %r' % message)\n",
    "    writer.write(message.encode())\n",
    "\n",
    "    data = await reader.read(100)\n",
    "    print('Received: %r' % data.decode())\n",
    "\n",
    "    print('Close the socket')\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "message = 'Hello World!'\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(tcp_echo_client(message, loop))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uname: Darwin, date: пятница,  2 марта 2018 г. 11:18:55 (MSK)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def run_command(*args):\n",
    "    # Create subprocess\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *args,\n",
    "        # stdout must a pipe to be accessible as process.stdout\n",
    "        stdout=asyncio.subprocess.PIPE)\n",
    "    # Wait for the subprocess to finish\n",
    "    stdout, stderr = await process.communicate()\n",
    "    # Return stdout\n",
    "    return stdout.decode().strip()\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "# Gather uname and date commands\n",
    "commands = asyncio.gather(run_command('uname'), run_command('date'))\n",
    "# Run the commands\n",
    "uname, date = loop.run_until_complete(commands)\n",
    "# Print a report\n",
    "print('uname: {}, date: {}'.format(uname, date))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exception handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import argparse\n",
    "import logging\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "import aiohttp\n",
    "import async_timeout\n",
    "\n",
    "\n",
    "LOGGER_FORMAT = '%(asctime)s %(message)s'\n",
    "URL_TEMPLATE = \"https://hacker-news.firebaseio.com/v0/item/{}.json\"\n",
    "TOP_STORIES_URL = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n",
    "FETCH_TIMEOUT = 10\n",
    "MAXIMUM_FETCHES = 5\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Calculate the number of comments of the top stories in HN.')\n",
    "parser.add_argument(\n",
    "    '--period', type=int, default=5, help='Number of seconds between poll')\n",
    "parser.add_argument(\n",
    "    '--limit', type=int, default=5,\n",
    "    help='Number of new stories to calculate comments for')\n",
    "parser.add_argument('--verbose', action='store_true', help='Detailed output')\n",
    "\n",
    "\n",
    "logging.basicConfig(format=LOGGER_FORMAT, datefmt='[%H:%M:%S]')\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class BoomException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class URLFetcher():\n",
    "    \"\"\"Provides counting of URL fetches for a particular task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.fetch_counter = 0\n",
    "\n",
    "    async def fetch(self, session, url):\n",
    "        \"\"\"Fetch a URL using aiohttp returning parsed JSON response.\n",
    "        As suggested by the aiohttp docs we reuse the session.\n",
    "        \"\"\"\n",
    "        with async_timeout.timeout(FETCH_TIMEOUT):\n",
    "            self.fetch_counter += 1\n",
    "            if self.fetch_counter > MAXIMUM_FETCHES:\n",
    "                raise BoomException('BOOM!')\n",
    "            elif randint(0, 3) == 0:\n",
    "                raise Exception('Random generic exception')\n",
    "\n",
    "            async with session.get(url) as response:\n",
    "                return await response.json()\n",
    "\n",
    "\n",
    "async def post_number_of_comments(loop, session, fetcher, post_id):\n",
    "    \"\"\"Retrieve data for current post and recursively for all comments.\n",
    "    \"\"\"\n",
    "    url = URL_TEMPLATE.format(post_id)\n",
    "    try:\n",
    "        response = await fetcher.fetch(session, url)\n",
    "    except BoomException as e:\n",
    "        log.error(\"Error retrieving post {}: {}\".format(post_id, e))\n",
    "        raise e\n",
    "\n",
    "    # base case, there are no comments\n",
    "    if response is None or 'kids' not in response:\n",
    "        return 0\n",
    "\n",
    "    # calculate this post's comments as number of comments\n",
    "    number_of_comments = len(response['kids'])\n",
    "\n",
    "    # create recursive tasks for all comments\n",
    "    tasks = [post_number_of_comments(\n",
    "        loop, session, fetcher, kid_id) for kid_id in response['kids']]\n",
    "\n",
    "    # schedule the tasks and retrieve results\n",
    "    try:\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    except BoomException as e:\n",
    "        log.error(\"Error retrieving comments for top stories: {}\".format(e))\n",
    "        raise\n",
    "\n",
    "    # reduce the descendents comments and add it to this post's\n",
    "    number_of_comments += sum(results)\n",
    "    log.debug('{:^6} > {} comments'.format(post_id, number_of_comments))\n",
    "\n",
    "    return number_of_comments\n",
    "\n",
    "\n",
    "async def get_comments_of_top_stories(loop, session, limit, iteration):\n",
    "    \"\"\"Retrieve top stories in HN.\n",
    "    \"\"\"\n",
    "    fetcher = URLFetcher()  # create a new fetcher for this task\n",
    "    try:\n",
    "        response = await fetcher.fetch(session, TOP_STORIES_URL)\n",
    "    except Exception as e:\n",
    "        log.error(\"Error retrieving top stories: {}\".format(e))\n",
    "        raise\n",
    "\n",
    "    tasks = [post_number_of_comments(\n",
    "        loop, session, fetcher, post_id) for post_id in response[:limit]]\n",
    "\n",
    "    try:\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    except Exception as e:\n",
    "        log.error(\"Error retrieving comments for top stories: {}\".format(e))\n",
    "        raise\n",
    "\n",
    "    for post_id, num_comments in zip(response[:limit], results):\n",
    "        log.info(\"Post {} has {} comments ({})\".format(\n",
    "            post_id, num_comments, iteration))\n",
    "    return fetcher.fetch_counter  # return the fetch count\n",
    "\n",
    "\n",
    "async def poll_top_stories_for_comments(loop, session, period, limit):\n",
    "    \"\"\"Periodically poll for new stories and retrieve number of comments.\n",
    "    \"\"\"\n",
    "    iteration = 1\n",
    "    errors = []\n",
    "    while True:\n",
    "        if errors:\n",
    "            log.info('Error detected, quitting')\n",
    "            return\n",
    "\n",
    "        log.info(\"Calculating comments for top {} stories. ({})\".format(\n",
    "            limit, iteration))\n",
    "\n",
    "        future = asyncio.ensure_future(\n",
    "            get_comments_of_top_stories(loop, session, limit, iteration))\n",
    "\n",
    "        now = datetime.now()\n",
    "\n",
    "        def callback(fut):\n",
    "            try:\n",
    "                fetch_count = fut.result()\n",
    "            except BoomException as e:\n",
    "                log.debug('Adding {} to errors'.format(e))\n",
    "                errors.append(e)\n",
    "            except Exception as e:\n",
    "                log.exception('Unexpected error')\n",
    "                errors.append(e)\n",
    "            else:\n",
    "                log.info(\n",
    "                    '> Calculating comments took {:.2f} seconds and {} fetches'.format(\n",
    "                        (datetime.now() - now).total_seconds(), fetch_count))\n",
    "\n",
    "        future.add_done_callback(callback)\n",
    "\n",
    "        log.info(\"Waiting for {} seconds...\".format(period))\n",
    "        iteration += 1\n",
    "        await asyncio.sleep(period)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    if args.verbose:\n",
    "        log.setLevel(logging.DEBUG)\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    with aiohttp.ClientSession(loop=loop) as session:\n",
    "        loop.run_until_complete(\n",
    "            poll_top_stories_for_comments(\n",
    "                loop, session, args.period, args.limit))\n",
    "\n",
    "    loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://flupy.org/resources/yield-from.pdf\n",
    "* https://www.python.org/dev/peps/pep-0380/#proposal\n",
    "* https://groups.google.com/forum/#!msg/python-tulip/bmphRrryuFk/aB45sEJUomYJ\n",
    "* https://www.youtube.com/watch?v=1coLC-MUCJc\n",
    "* https://stackoverflow.com/questions/40571786/asyncio-coroutine-vs-async-def\n",
    "* https://lwn.net/Articles/643786/\n",
    "* https://hackernoon.com/asyncio-for-the-working-python-developer-5c468e6e2e8e\n",
    "* https://medium.com/python-pandemonium/asyncio-coroutine-patterns-beyond-await-a6121486656f\n",
    "* https://medium.com/@yeraydiazdiaz/asyncio-coroutine-patterns-errors-and-cancellation-3bb422e961ff\n",
    "* http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html\n",
    "* https://asyncio.readthedocs.io/en/latest/index.html\n",
    "* http://lucumr.pocoo.org/2016/10/30/i-dont-understand-asyncio/\n",
    "* https://snarky.ca/how-the-heck-does-async-await-work-in-python-3-5/\n",
    "* https://vorpus.org/blog/some-thoughts-on-asynchronous-api-design-in-a-post-asyncawait-world/\n",
    "* https://www.pythonsheets.com/notes/python-asyncio.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* new asyncIO thing is quite powerful\n",
    "* async is a framework doing all heavy lifting you don't need to worry about\n",
    "* don't block event loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
