{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #12. Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Multithreading](#Multithreading)\n",
    "2. [Multiprocessing](#Multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concur](https://pbs.twimg.com/media/CCU6tQbWgAAtm7M.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parall](https://birdchan.files.wordpress.com/2017/05/concurrency_vs_parallelism.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![amd](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/AmdahlsLaw.svg/640px-AmdahlsLaw.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each additional process will increase the communication overhead and decrease the available RAM, so you rarely get a full nx speedup. Depending on which problem you are solving, the communication overhead can even get so large that you can see very significant slowdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperthreading can give up to a 30% performance gain if there are enough spare computing resources. This works if, for example, you have a mix of floating-point and integer arithmetic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split your jobs into independent units of work.\n",
    "* If your workers take varying amounts of time, then consider randomizing the sequence of work (another example would be for processing variable-sized files).\n",
    "* Sorting your work queue so slowest jobs go first maybe an equally useful strategy.\n",
    "* Use the default chunksize unless you have verified reasons for adjusting it.\n",
    "* Align the number of jobs with the number of physical CPUs (again, the default chunksize takes care of this for you, although it will use any hyperthreads by default, which may not offer any additional gain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write done by Thread-10\n",
      "URL http://www.facebook.com fetched by Thread-10\n",
      "write done by Thread-10\n",
      "URL http://www.google.com fetched by Thread-10\n",
      "write done by Thread-11\n",
      "URL http://www.youtube.com fetched by Thread-11\n",
      "write done by Thread-11\n",
      "URL http://www.yahoo.com fetched by Thread-11\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import threading\n",
    "\n",
    "class FetchUrls(threading.Thread):\n",
    "    \"\"\"\n",
    "    Thread checking URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, urls, output):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param urls list of urls to check\n",
    "        @param output file to write urls output\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.urls = urls\n",
    "        self.output = output\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Check URLs one by one.\n",
    "        \"\"\"\n",
    "        while self.urls:\n",
    "            url = self.urls.pop()\n",
    "            req = urllib2.Request(url)\n",
    "            try:\n",
    "                d = urllib2.urlopen(req)\n",
    "            except urllib2.URLError, e:\n",
    "                print 'URL %s failed: %s' % (url, e.reason)\n",
    "            self.output.write(d.read())\n",
    "            print 'write done by %s' % self.name\n",
    "            print 'URL %s fetched by %s' % (url, self.name)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # list 1 of urls to fetch\n",
    "    urls1 = ['http://www.google.com', 'http://www.facebook.com']\n",
    "    # list 2 of urls to fetch\n",
    "    urls2 = ['http://www.yahoo.com', 'http://www.youtube.com']\n",
    "    f = open('output.txt', 'w+')\n",
    "    t1 = FetchUrls(urls1, f)\n",
    "    t2 = FetchUrls(urls2, f)\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lock acquired by Thread-14\n",
      "write done by Thread-14\n",
      "lock released by Thread-14\n",
      "URL http://www.youtube.com fetched by Thread-14lock acquired by Thread-13\n",
      "\n",
      "write done by Thread-13\n",
      "lock released by Thread-13\n",
      "URL http://www.facebook.com fetched by Thread-13\n",
      "lock acquired by Thread-13\n",
      "write done by Thread-13\n",
      "lock released by Thread-13\n",
      "URL http://www.google.com fetched by Thread-13\n",
      "lock acquired by Thread-14\n",
      "write done by Thread-14\n",
      "lock released by Thread-14\n",
      "URL http://www.yahoo.com fetched by Thread-14\n"
     ]
    }
   ],
   "source": [
    "class FetchUrls(threading.Thread):\n",
    "\n",
    "    def __init__(self, urls, output, lock):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.urls = urls\n",
    "        self.output = output\n",
    "        self.lock = lock\n",
    "    \n",
    "    def run(self):\n",
    "        while self.urls:\n",
    "            url = self.urls.pop()\n",
    "            req = urllib2.Request(url)\n",
    "            try:\n",
    "                d = urllib2.urlopen(req)\n",
    "            except urllib2.URLError, e:\n",
    "                print 'URL %s failed: %s' % (url, e.reason)\n",
    "            self.lock.acquire()\n",
    "            print 'lock acquired by %s' % self.name\n",
    "            self.output.write(d.read())\n",
    "            print 'write done by %s' % self.name\n",
    "            print 'lock released by %s' % self.name\n",
    "            self.lock.release()\n",
    "            print 'URL %s fetched by %s' % (url, self.name)\n",
    "\n",
    "    # same as above but using `with`\n",
    "    def run_with_lock(self):\n",
    "        while self.urls:\n",
    "            url = self.urls.pop()\n",
    "            req = urllib2.Request(url)\n",
    "            try:\n",
    "                d = urllib2.urlopen(req)\n",
    "            except urllib2.URLError, e:\n",
    "                print 'URL %s failed: %s' % (url, e.reason)\n",
    "            with self.lock:\n",
    "                print 'lock acquired by %s' % self.name\n",
    "                self.output.write(d.read())\n",
    "                print 'write done by %s' % self.name\n",
    "            print 'lock released by %s' % self.name\n",
    "            print 'URL %s fetched by %s' % (url, self.name)\n",
    "            \n",
    "def main():\n",
    "    lock = threading.Lock()\n",
    "    urls1 = ['http://www.google.com', 'http://www.facebook.com']\n",
    "    urls2 = ['http://www.yahoo.com', 'http://www.youtube.com']\n",
    "    f = open('output.txt', 'w+')\n",
    "    t1 = FetchUrls(urls1, f, lock)\n",
    "    t2 = FetchUrls(urls2, f, lock)\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lock internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python/thread_pthread.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyThread_type_lock\n",
    "PyThread_allocate_lock(void)\n",
    "{\n",
    "    sem_t *lock;\n",
    "    int status, error = 0;\n",
    "\n",
    "    dprintf((\"PyThread_allocate_lock called\\n\"));\n",
    "    if (!initialized)\n",
    "        PyThread_init_thread();\n",
    "\n",
    "    lock = (sem_t *)malloc(sizeof(sem_t));\n",
    "\n",
    "    if (lock) {\n",
    "        status = sem_init(lock,0,1);\n",
    "        CHECK_STATUS(\"sem_init\");\n",
    "\n",
    "        if (error) {\n",
    "            free((void *)lock);\n",
    "            lock = NULL;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    dprintf((\"PyThread_allocate_lock() -> %p\\n\", lock));\n",
    "    return (PyThread_type_lock)lock;\n",
    "}\n",
    "\n",
    "void\n",
    "PyThread_free_lock(PyThread_type_lock lock)\n",
    "{\n",
    "    sem_t *thelock = (sem_t *)lock;\n",
    "    int status, error = 0;\n",
    "\n",
    "    (void) error; /* silence unused-but-set-variable warning */\n",
    "    dprintf((\"PyThread_free_lock(%p) called\\n\", lock));\n",
    "\n",
    "    if (!thelock)\n",
    "        return;\n",
    "\n",
    "    status = sem_destroy(thelock);\n",
    "    CHECK_STATUS(\"sem_destroy\");\n",
    "\n",
    "    free((void *)thelock);\n",
    "}\n",
    "\n",
    "int\n",
    "PyThread_acquire_lock(PyThread_type_lock lock, int waitflag)\n",
    "{\n",
    "    int success;\n",
    "    sem_t *thelock = (sem_t *)lock;\n",
    "    int status, error = 0;\n",
    "\n",
    "    (void) error; /* silence unused-but-set-variable warning */\n",
    "    dprintf((\"PyThread_acquire_lock(%p, %d) called\\n\", lock, waitflag));\n",
    "\n",
    "    do {\n",
    "        if (waitflag)\n",
    "            status = fix_status(sem_wait(thelock));\n",
    "        else\n",
    "            status = fix_status(sem_trywait(thelock));\n",
    "    } while (status == EINTR); /* Retry if interrupted by a signal */\n",
    "\n",
    "    if (waitflag) {\n",
    "        CHECK_STATUS(\"sem_wait\");\n",
    "    } else if (status != EAGAIN) {\n",
    "        CHECK_STATUS(\"sem_trywait\");\n",
    "    }\n",
    "\n",
    "    success = (status == 0) ? 1 : 0;\n",
    "\n",
    "    dprintf((\"PyThread_acquire_lock(%p, %d) -> %d\\n\", lock, waitflag, success));\n",
    "    return success;\n",
    "}\n",
    "\n",
    "void\n",
    "PyThread_release_lock(PyThread_type_lock lock)\n",
    "{\n",
    "    sem_t *thelock = (sem_t *)lock;\n",
    "    int status, error = 0;\n",
    "\n",
    "    (void) error; /* silence unused-but-set-variable warning */\n",
    "    dprintf((\"PyThread_release_lock(%p) called\\n\", lock));\n",
    "\n",
    "    status = sem_post(thelock);\n",
    "    CHECK_STATUS(\"sem_post\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLock(*args, **kwargs):\n",
    "    \"\"\"Factory function that returns a new reentrant lock.\n",
    "    A reentrant lock must be released by the thread that acquired it. Once a\n",
    "    thread has acquired a reentrant lock, the same thread may acquire it again\n",
    "    without blocking; the thread must release it once for each time it has\n",
    "    acquired it.\n",
    "    \"\"\"\n",
    "    return _RLock(*args, **kwargs)\n",
    "\n",
    "class _RLock(_Verbose):\n",
    "    \"\"\"A reentrant lock must be released by the thread that acquired it. Once a\n",
    "       thread has acquired a reentrant lock, the same thread may acquire it\n",
    "       again without blocking; the thread must release it once for each time it\n",
    "       has acquired it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=None):\n",
    "        _Verbose.__init__(self, verbose)\n",
    "        self.__block = _allocate_lock()\n",
    "        self.__owner = None\n",
    "        self.__count = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        owner = self.__owner\n",
    "        try:\n",
    "            owner = _active[owner].name\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return \"<%s owner=%r count=%d>\" % (\n",
    "                self.__class__.__name__, owner, self.__count)\n",
    "\n",
    "    def acquire(self, blocking=1):\n",
    "        \"\"\"Acquire a lock, blocking or non-blocking.\n",
    "        When invoked without arguments: if this thread already owns the lock,\n",
    "        increment the recursion level by one, and return immediately. Otherwise,\n",
    "        if another thread owns the lock, block until the lock is unlocked. Once\n",
    "        the lock is unlocked (not owned by any thread), then grab ownership, set\n",
    "        the recursion level to one, and return. If more than one thread is\n",
    "        blocked waiting until the lock is unlocked, only one at a time will be\n",
    "        able to grab ownership of the lock. There is no return value in this\n",
    "        case.\n",
    "        When invoked with the blocking argument set to true, do the same thing\n",
    "        as when called without arguments, and return true.\n",
    "        When invoked with the blocking argument set to false, do not block. If a\n",
    "        call without an argument would block, return false immediately;\n",
    "        otherwise, do the same thing as when called without arguments, and\n",
    "        return true.\n",
    "        \"\"\"\n",
    "        me = _get_ident()\n",
    "        if self.__owner == me:\n",
    "            self.__count = self.__count + 1\n",
    "            if __debug__:\n",
    "                self._note(\"%s.acquire(%s): recursive success\", self, blocking)\n",
    "            return 1\n",
    "        rc = self.__block.acquire(blocking)\n",
    "        if rc:\n",
    "            self.__owner = me\n",
    "            self.__count = 1\n",
    "            if __debug__:\n",
    "                self._note(\"%s.acquire(%s): initial success\", self, blocking)\n",
    "        else:\n",
    "            if __debug__:\n",
    "                self._note(\"%s.acquire(%s): failure\", self, blocking)\n",
    "        return rc\n",
    "\n",
    "    __enter__ = acquire\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\"Release a lock, decrementing the recursion level.\n",
    "        If after the decrement it is zero, reset the lock to unlocked (not owned\n",
    "        by any thread), and if any other threads are blocked waiting for the\n",
    "        lock to become unlocked, allow exactly one of them to proceed. If after\n",
    "        the decrement the recursion level is still nonzero, the lock remains\n",
    "        locked and owned by the calling thread.\n",
    "        Only call this method when the calling thread owns the lock. A\n",
    "        RuntimeError is raised if this method is called when the lock is\n",
    "        unlocked.\n",
    "        There is no return value.\n",
    "        \"\"\"\n",
    "        if self.__owner != _get_ident():\n",
    "            raise RuntimeError(\"cannot release un-acquired lock\")\n",
    "        self.__count = count = self.__count - 1\n",
    "        if not count:\n",
    "            self.__owner = None\n",
    "            self.__block.release()\n",
    "            if __debug__:\n",
    "                self._note(\"%s.release(): final release\", self)\n",
    "        else:\n",
    "            if __debug__:\n",
    "                self._note(\"%s.release(): non-final release\", self)\n",
    "\n",
    "    def __exit__(self, t, v, tb):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Producer(threading.Thread):\n",
    "    \"\"\"\n",
    "    Produces random integers to a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers, condition):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param integers list of integers\n",
    "        @param condition condition synchronization object\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.integers = integers\n",
    "        self.condition = condition\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Append random integers to the integers list\n",
    "        at random time.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            integer = random.randint(0, 256)\n",
    "            self.condition.acquire()\n",
    "            print 'condition acquired by %s' % self.name\n",
    "            self.integers.append(integer) \n",
    "            print '%d appended to list by %s' % (integer, self.name)\n",
    "            print 'condition notified by %s' % self.name\n",
    "            self.condition.notify()\n",
    "            print 'condition released by %s' % self.name\n",
    "            self.condition.release()\n",
    "            time.sleep(1)\n",
    "\n",
    "            \n",
    "class Consumer(threading.Thread):\n",
    "    \"\"\"\n",
    "    Consumes random integers from a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers, condition):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param integers list of integers\n",
    "        @param condition condition synchronization object\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.integers = integers\n",
    "        self.condition = condition\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Consumes integers from list\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            self.condition.acquire()\n",
    "            print 'condition acquired by %s' % self.name\n",
    "            while True:\n",
    "                if self.integers:\n",
    "                    integer = self.integers.pop()\n",
    "                    print '%d popped from list by %s' % (integer, self.name)\n",
    "                    break\n",
    "                print 'condition wait by %s' % self.name\n",
    "                self.condition.wait()\n",
    "            print 'condition released by %s' % self.name\n",
    "            self.condition.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Condition(_Verbose):\n",
    "    \"\"\"Condition variables allow one or more threads to wait until they are\n",
    "       notified by another thread.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lock=None, verbose=None):\n",
    "        _Verbose.__init__(self, verbose)\n",
    "        if lock is None:\n",
    "            lock = RLock()\n",
    "        self.__lock = lock\n",
    "        # Export the lock's acquire() and release() methods\n",
    "        self.acquire = lock.acquire\n",
    "        self.release = lock.release\n",
    "        # If the lock defines _release_save() and/or _acquire_restore(),\n",
    "        # these override the default implementations (which just call\n",
    "        # release() and acquire() on the lock).  Ditto for _is_owned().\n",
    "        try:\n",
    "            self._release_save = lock._release_save\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        try:\n",
    "            self._acquire_restore = lock._acquire_restore\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        try:\n",
    "            self._is_owned = lock._is_owned\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        self.__waiters = []\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.__lock.__enter__()\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        return self.__lock.__exit__(*args)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Condition(%s, %d)>\" % (self.__lock, len(self.__waiters))\n",
    "\n",
    "    def _release_save(self):\n",
    "        self.__lock.release()           # No state to save\n",
    "\n",
    "    def _acquire_restore(self, x):\n",
    "        self.__lock.acquire()           # Ignore saved state\n",
    "\n",
    "    def _is_owned(self):\n",
    "        # Return True if lock is owned by current_thread.\n",
    "        # This method is called only if __lock doesn't have _is_owned().\n",
    "        if self.__lock.acquire(0):\n",
    "            self.__lock.release()\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def wait(self, timeout=None):\n",
    "        \"\"\"Wait until notified or until a timeout occurs.\n",
    "        If the calling thread has not acquired the lock when this method is\n",
    "        called, a RuntimeError is raised.\n",
    "        This method releases the underlying lock, and then blocks until it is\n",
    "        awakened by a notify() or notifyAll() call for the same condition\n",
    "        variable in another thread, or until the optional timeout occurs. Once\n",
    "        awakened or timed out, it re-acquires the lock and returns.\n",
    "        When the timeout argument is present and not None, it should be a\n",
    "        floating point number specifying a timeout for the operation in seconds\n",
    "        (or fractions thereof).\n",
    "        When the underlying lock is an RLock, it is not released using its\n",
    "        release() method, since this may not actually unlock the lock when it\n",
    "        was acquired multiple times recursively. Instead, an internal interface\n",
    "        of the RLock class is used, which really unlocks it even when it has\n",
    "        been recursively acquired several times. Another internal interface is\n",
    "        then used to restore the recursion level when the lock is reacquired.\n",
    "        \"\"\"\n",
    "        if not self._is_owned():\n",
    "            raise RuntimeError(\"cannot wait on un-acquired lock\")\n",
    "        waiter = _allocate_lock()\n",
    "        waiter.acquire()\n",
    "        self.__waiters.append(waiter)\n",
    "        saved_state = self._release_save()\n",
    "        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n",
    "            if timeout is None:\n",
    "                waiter.acquire()\n",
    "                if __debug__:\n",
    "                    self._note(\"%s.wait(): got it\", self)\n",
    "            else:\n",
    "                # Balancing act:  We can't afford a pure busy loop, so we\n",
    "                # have to sleep; but if we sleep the whole timeout time,\n",
    "                # we'll be unresponsive.  The scheme here sleeps very\n",
    "                # little at first, longer as time goes on, but never longer\n",
    "                # than 20 times per second (or the timeout time remaining).\n",
    "                endtime = _time() + timeout\n",
    "                delay = 0.0005 # 500 us -> initial delay of 1 ms\n",
    "                while True:\n",
    "                    gotit = waiter.acquire(0)\n",
    "                    if gotit:\n",
    "                        break\n",
    "                    remaining = endtime - _time()\n",
    "                    if remaining <= 0:\n",
    "                        break\n",
    "                    delay = min(delay * 2, remaining, .05)\n",
    "                    _sleep(delay)\n",
    "                if not gotit:\n",
    "                    if __debug__:\n",
    "                        self._note(\"%s.wait(%s): timed out\", self, timeout)\n",
    "                    try:\n",
    "                        self.__waiters.remove(waiter)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                else:\n",
    "                    if __debug__:\n",
    "                        self._note(\"%s.wait(%s): got it\", self, timeout)\n",
    "        finally:\n",
    "            self._acquire_restore(saved_state)\n",
    "\n",
    "    def notify(self, n=1):\n",
    "        \"\"\"Wake up one or more threads waiting on this condition, if any.\n",
    "        If the calling thread has not acquired the lock when this method is\n",
    "        called, a RuntimeError is raised.\n",
    "        This method wakes up at most n of the threads waiting for the condition\n",
    "        variable; it is a no-op if no threads are waiting.\n",
    "        \"\"\"\n",
    "        if not self._is_owned():\n",
    "            raise RuntimeError(\"cannot notify on un-acquired lock\")\n",
    "        __waiters = self.__waiters\n",
    "        waiters = __waiters[:n]\n",
    "        if not waiters:\n",
    "            if __debug__:\n",
    "                self._note(\"%s.notify(): no waiters\", self)\n",
    "            return\n",
    "        self._note(\"%s.notify(): notifying %d waiter%s\", self, n,\n",
    "                   n!=1 and \"s\" or \"\")\n",
    "        for waiter in waiters:\n",
    "            waiter.release()\n",
    "            try:\n",
    "                __waiters.remove(waiter)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    def notifyAll(self):\n",
    "        \"\"\"Wake up all threads waiting on this condition.\n",
    "        If the calling thread has not acquired the lock when this method\n",
    "        is called, a RuntimeError is raised.\n",
    "        \"\"\"\n",
    "        self.notify(len(self.__waiters))\n",
    "\n",
    "    notify_all = notifyAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semaphore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Semaphore(_Verbose):\n",
    "    \"\"\"Semaphores manage a counter representing the number of release() calls\n",
    "       minus the number of acquire() calls, plus an initial value. The acquire()\n",
    "       method blocks if necessary until it can return without making the counter\n",
    "       negative. If not given, value defaults to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n",
    "\n",
    "    def __init__(self, value=1, verbose=None):\n",
    "        if value < 0:\n",
    "            raise ValueError(\"semaphore initial value must be >= 0\")\n",
    "        _Verbose.__init__(self, verbose)\n",
    "        self.__cond = Condition(Lock())\n",
    "        self.__value = value\n",
    "\n",
    "    def acquire(self, blocking=1):\n",
    "        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n",
    "        When invoked without arguments: if the internal counter is larger than\n",
    "        zero on entry, decrement it by one and return immediately. If it is zero\n",
    "        on entry, block, waiting until some other thread has called release() to\n",
    "        make it larger than zero. This is done with proper interlocking so that\n",
    "        if multiple acquire() calls are blocked, release() will wake exactly one\n",
    "        of them up. The implementation may pick one at random, so the order in\n",
    "        which blocked threads are awakened should not be relied on. There is no\n",
    "        return value in this case.\n",
    "        When invoked with blocking set to true, do the same thing as when called\n",
    "        without arguments, and return true.\n",
    "        When invoked with blocking set to false, do not block. If a call without\n",
    "        an argument would block, return false immediately; otherwise, do the\n",
    "        same thing as when called without arguments, and return true.\n",
    "        \"\"\"\n",
    "        rc = False\n",
    "        with self.__cond:\n",
    "            while self.__value == 0:\n",
    "                if not blocking:\n",
    "                    break\n",
    "                if __debug__:\n",
    "                    self._note(\"%s.acquire(%s): blocked waiting, value=%s\",\n",
    "                            self, blocking, self.__value)\n",
    "                self.__cond.wait()\n",
    "            else:\n",
    "                self.__value = self.__value - 1\n",
    "                if __debug__:\n",
    "                    self._note(\"%s.acquire: success, value=%s\",\n",
    "                            self, self.__value)\n",
    "                rc = True\n",
    "        return rc\n",
    "\n",
    "    __enter__ = acquire\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\"Release a semaphore, incrementing the internal counter by one.\n",
    "        When the counter is zero on entry and another thread is waiting for it\n",
    "        to become larger than zero again, wake up that thread.\n",
    "        \"\"\"\n",
    "        with self.__cond:\n",
    "            self.__value = self.__value + 1\n",
    "            if __debug__:\n",
    "                self._note(\"%s.release: success, value=%s\",\n",
    "                        self, self.__value)\n",
    "            self.__cond.notify()\n",
    "\n",
    "    def __exit__(self, t, v, tb):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Producer(threading.Thread):\n",
    "    \"\"\"\n",
    "    Produces random integers to a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers, event):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param integers list of integers\n",
    "        @param event event synchronization object\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.integers = integers\n",
    "        self.event = event\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Append random integers to the integers list\n",
    "        at random time.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            integer = random.randint(0, 256)\n",
    "            self.integers.append(integer) \n",
    "            print '%d appended to list by %s' % (integer, self.name)\n",
    "            print 'event set by %s' % self.name\n",
    "            self.event.set()\n",
    "            self.event.clear()\n",
    "            print 'event cleared by %s' % self.name\n",
    "            time.sleep(1)\n",
    "\n",
    "            \n",
    "class Consumer(threading.Thread):\n",
    "    \"\"\"\n",
    "    Consumes random integers from a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, integers, event):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param integers list of integers\n",
    "        @param event event synchronization object\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.integers = integers\n",
    "        self.event = event\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Consumes integers from list\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            self.event.wait()\n",
    "            try:\n",
    "                integer = self.integers.pop()\n",
    "                print '%d popped from list by %s' % (integer, self.name)\n",
    "            except IndexError:\n",
    "                # catch pop on empty list\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "SHOULD_EXIT = threading.Event()\n",
    "DB_LOCK = threading.Lock()\n",
    "\n",
    "def check_for_exit():\n",
    "    if SHOULD_EXIT.is_set():\n",
    "        raise SystemExit\n",
    "\n",
    "\n",
    "def loader_cycle(config, db_config, worker_name):\n",
    "    # ...\n",
    "    # skipped code\n",
    "    # ...\n",
    "\n",
    "    while True:\n",
    "        tmp_fullname = None\n",
    "        handled_files = None\n",
    "        remote_fns = None\n",
    "        task_id = None\n",
    "        try:\n",
    "            check_for_exit()\n",
    "            try:\n",
    "                task = acquire_task(config, worker_name, host_name)\n",
    "            except Exception, e:\n",
    "                logging.warning(\"Cannot acquire task: %s\", repr(e))\n",
    "                task = None\n",
    "\n",
    "            if task is None:\n",
    "                SHOULD_EXIT.wait(config['idle_sleep'])\n",
    "                check_for_exit()\n",
    "                continue\n",
    "    # ...\n",
    "    # skipped code\n",
    "    # ...\n",
    "\n",
    "def send_heartbeat(conn, host):\n",
    "    update_query = \"\"\"\n",
    "        REPLACE INTO `servers`\n",
    "        SET `host` = %s, `heartbeat` = NOW()\n",
    "        \"\"\"\n",
    "    try:\n",
    "        with DB_LOCK:\n",
    "            conn.execute(update_query, host)\n",
    "    except Exception, e:\n",
    "        logging.warning(\"Cannot send heartbeat: %s\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _Event(_Verbose):\n",
    "    \"\"\"A factory function that returns a new event object. An event manages a\n",
    "       flag that can be set to true with the set() method and reset to false\n",
    "       with the clear() method. The wait() method blocks until the flag is true.\n",
    "    \"\"\"\n",
    "\n",
    "    # After Tim Peters' event class (without is_posted())\n",
    "\n",
    "    def __init__(self, verbose=None):\n",
    "        _Verbose.__init__(self, verbose)\n",
    "        self.__cond = Condition(Lock())\n",
    "        self.__flag = False\n",
    "\n",
    "    def _reset_internal_locks(self):\n",
    "        # private!  called by Thread._reset_internal_locks by _after_fork()\n",
    "        self.__cond.__init__(Lock())\n",
    "\n",
    "    def isSet(self):\n",
    "        'Return true if and only if the internal flag is true.'\n",
    "        return self.__flag\n",
    "\n",
    "    is_set = isSet\n",
    "\n",
    "    def set(self):\n",
    "        \"\"\"Set the internal flag to true.\n",
    "        All threads waiting for the flag to become true are awakened. Threads\n",
    "        that call wait() once the flag is true will not block at all.\n",
    "        \"\"\"\n",
    "        with self.__cond:\n",
    "            self.__flag = True\n",
    "            self.__cond.notify_all()\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Reset the internal flag to false.\n",
    "        Subsequently, threads calling wait() will block until set() is called to\n",
    "        set the internal flag to true again.\n",
    "        \"\"\"\n",
    "        with self.__cond:\n",
    "            self.__flag = False\n",
    "\n",
    "    def wait(self, timeout=None):\n",
    "        \"\"\"Block until the internal flag is true.\n",
    "        If the internal flag is true on entry, return immediately. Otherwise,\n",
    "        block until another thread calls set() to set the flag to true, or until\n",
    "        the optional timeout occurs.\n",
    "        When the timeout argument is present and not None, it should be a\n",
    "        floating point number specifying a timeout for the operation in seconds\n",
    "        (or fractions thereof).\n",
    "        This method returns the internal flag on exit, so it will always return\n",
    "        True except if a timeout is given and the operation times out.\n",
    "        \"\"\"\n",
    "        with self.__cond:\n",
    "            if not self.__flag:\n",
    "                self.__cond.wait(timeout)\n",
    "            return self.__flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Producer(threading.Thread):\n",
    "    \"\"\"\n",
    "    Produces random integers to a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, queue):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param integers list of integers\n",
    "        @param queue queue synchronization object\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Append random integers to the integers list at\n",
    "        random time.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            integer = random.randint(0, 256)\n",
    "            self.queue.put(integer) \n",
    "            print '%d put to queue by %s' % (integer, self.name)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            \n",
    "class Consumer(threading.Thread):\n",
    "    \"\"\"\n",
    "    Consumes random integers from a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, queue):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        @param integers list of integers\n",
    "        @param queue queue synchronization object\n",
    "        \"\"\"\n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Thread run method. Consumes integers from list\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            integer = self.queue.get()\n",
    "            print '%d popped from list by %s' % (integer, self.name)\n",
    "            self.queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queue:\n",
    "    \"\"\"Create a queue object with a given maximum size.\n",
    "    If maxsize is <= 0, the queue size is infinite.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxsize=0):\n",
    "        self.maxsize = maxsize\n",
    "        self._init(maxsize)\n",
    "        # mutex must be held whenever the queue is mutating.  All methods\n",
    "        # that acquire mutex must release it before returning.  mutex\n",
    "        # is shared between the three conditions, so acquiring and\n",
    "        # releasing the conditions also acquires and releases mutex.\n",
    "        self.mutex = _threading.Lock()\n",
    "        # Notify not_empty whenever an item is added to the queue; a\n",
    "        # thread waiting to get is notified then.\n",
    "        self.not_empty = _threading.Condition(self.mutex)\n",
    "        # Notify not_full whenever an item is removed from the queue;\n",
    "        # a thread waiting to put is notified then.\n",
    "        self.not_full = _threading.Condition(self.mutex)\n",
    "        # Notify all_tasks_done whenever the number of unfinished tasks\n",
    "        # drops to zero; thread waiting to join() is notified to resume\n",
    "        self.all_tasks_done = _threading.Condition(self.mutex)\n",
    "        self.unfinished_tasks = 0\n",
    "\n",
    "    def task_done(self):\n",
    "        \"\"\"Indicate that a formerly enqueued task is complete.\n",
    "        Used by Queue consumer threads.  For each get() used to fetch a task,\n",
    "        a subsequent call to task_done() tells the queue that the processing\n",
    "        on the task is complete.\n",
    "        If a join() is currently blocking, it will resume when all items\n",
    "        have been processed (meaning that a task_done() call was received\n",
    "        for every item that had been put() into the queue).\n",
    "        Raises a ValueError if called more times than there were items\n",
    "        placed in the queue.\n",
    "        \"\"\"\n",
    "        self.all_tasks_done.acquire()\n",
    "        try:\n",
    "            unfinished = self.unfinished_tasks - 1\n",
    "            if unfinished <= 0:\n",
    "                if unfinished < 0:\n",
    "                    raise ValueError('task_done() called too many times')\n",
    "                self.all_tasks_done.notify_all()\n",
    "            self.unfinished_tasks = unfinished\n",
    "        finally:\n",
    "            self.all_tasks_done.release()\n",
    "\n",
    "    def join(self):\n",
    "        \"\"\"Blocks until all items in the Queue have been gotten and processed.\n",
    "        The count of unfinished tasks goes up whenever an item is added to the\n",
    "        queue. The count goes down whenever a consumer thread calls task_done()\n",
    "        to indicate the item was retrieved and all work on it is complete.\n",
    "        When the count of unfinished tasks drops to zero, join() unblocks.\n",
    "        \"\"\"\n",
    "        self.all_tasks_done.acquire()\n",
    "        try:\n",
    "            while self.unfinished_tasks:\n",
    "                self.all_tasks_done.wait()\n",
    "        finally:\n",
    "            self.all_tasks_done.release()\n",
    "\n",
    "    def qsize(self):\n",
    "        \"\"\"Return the approximate size of the queue (not reliable!).\"\"\"\n",
    "        self.mutex.acquire()\n",
    "        n = self._qsize()\n",
    "        self.mutex.release()\n",
    "        return n\n",
    "\n",
    "    def empty(self):\n",
    "        \"\"\"Return True if the queue is empty, False otherwise (not reliable!).\"\"\"\n",
    "        self.mutex.acquire()\n",
    "        n = not self._qsize()\n",
    "        self.mutex.release()\n",
    "        return n\n",
    "\n",
    "    def full(self):\n",
    "        \"\"\"Return True if the queue is full, False otherwise (not reliable!).\"\"\"\n",
    "        self.mutex.acquire()\n",
    "        n = 0 < self.maxsize == self._qsize()\n",
    "        self.mutex.release()\n",
    "        return n\n",
    "\n",
    "    def put(self, item, block=True, timeout=None):\n",
    "        \"\"\"Put an item into the queue.\n",
    "        If optional args 'block' is true and 'timeout' is None (the default),\n",
    "        block if necessary until a free slot is available. If 'timeout' is\n",
    "        a non-negative number, it blocks at most 'timeout' seconds and raises\n",
    "        the Full exception if no free slot was available within that time.\n",
    "        Otherwise ('block' is false), put an item on the queue if a free slot\n",
    "        is immediately available, else raise the Full exception ('timeout'\n",
    "        is ignored in that case).\n",
    "        \"\"\"\n",
    "        self.not_full.acquire()\n",
    "        try:\n",
    "            if self.maxsize > 0:\n",
    "                if not block:\n",
    "                    if self._qsize() == self.maxsize:\n",
    "                        raise Full\n",
    "                elif timeout is None:\n",
    "                    while self._qsize() == self.maxsize:\n",
    "                        self.not_full.wait()\n",
    "                elif timeout < 0:\n",
    "                    raise ValueError(\"'timeout' must be a non-negative number\")\n",
    "                else:\n",
    "                    endtime = _time() + timeout\n",
    "                    while self._qsize() == self.maxsize:\n",
    "                        remaining = endtime - _time()\n",
    "                        if remaining <= 0.0:\n",
    "                            raise Full\n",
    "                        self.not_full.wait(remaining)\n",
    "            self._put(item)\n",
    "            self.unfinished_tasks += 1\n",
    "            self.not_empty.notify()\n",
    "        finally:\n",
    "            self.not_full.release()\n",
    "\n",
    "    def put_nowait(self, item):\n",
    "        \"\"\"Put an item into the queue without blocking.\n",
    "        Only enqueue the item if a free slot is immediately available.\n",
    "        Otherwise raise the Full exception.\n",
    "        \"\"\"\n",
    "        return self.put(item, False)\n",
    "\n",
    "    def get(self, block=True, timeout=None):\n",
    "        \"\"\"Remove and return an item from the queue.\n",
    "        If optional args 'block' is true and 'timeout' is None (the default),\n",
    "        block if necessary until an item is available. If 'timeout' is\n",
    "        a non-negative number, it blocks at most 'timeout' seconds and raises\n",
    "        the Empty exception if no item was available within that time.\n",
    "        Otherwise ('block' is false), return an item if one is immediately\n",
    "        available, else raise the Empty exception ('timeout' is ignored\n",
    "        in that case).\n",
    "        \"\"\"\n",
    "        self.not_empty.acquire()\n",
    "        try:\n",
    "            if not block:\n",
    "                if not self._qsize():\n",
    "                    raise Empty\n",
    "            elif timeout is None:\n",
    "                while not self._qsize():\n",
    "                    self.not_empty.wait()\n",
    "            elif timeout < 0:\n",
    "                raise ValueError(\"'timeout' must be a non-negative number\")\n",
    "            else:\n",
    "                endtime = _time() + timeout\n",
    "                while not self._qsize():\n",
    "                    remaining = endtime - _time()\n",
    "                    if remaining <= 0.0:\n",
    "                        raise Empty\n",
    "                    self.not_empty.wait(remaining)\n",
    "            item = self._get()\n",
    "            self.not_full.notify()\n",
    "            return item\n",
    "        finally:\n",
    "            self.not_empty.release()\n",
    "\n",
    "    def get_nowait(self):\n",
    "        \"\"\"Remove and return an item from the queue without blocking.\n",
    "        Only get an item if one is immediately available. Otherwise\n",
    "        raise the Empty exception.\n",
    "        \"\"\"\n",
    "        return self.get(False)\n",
    "\n",
    "    # Override these methods to implement other queue organizations\n",
    "    # (e.g. stack or priority queue).\n",
    "    # These will only be called with appropriate locks held\n",
    "\n",
    "    # Initialize the queue representation\n",
    "    def _init(self, maxsize):\n",
    "        self.queue = deque()\n",
    "\n",
    "    def _qsize(self, len=len):\n",
    "        return len(self.queue)\n",
    "\n",
    "    # Put a new item in the queue\n",
    "    def _put(self, item):\n",
    "        self.q.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import threading \n",
    "import Queue \n",
    "import urllib2 \n",
    "\n",
    "class Fetcher(threading.Thread): \n",
    "    def __init__(self, queue): \n",
    "        threading.Thread.__init__(self)\n",
    "        self._queue = queue \n",
    "\n",
    "    def run(self):\n",
    "        while True: \n",
    "            content = self._queue.get() \n",
    "            if isinstance(content, str) and content == 'quit':\n",
    "                break\n",
    "            response = urllib2.urlopen(content)\n",
    "        print 'Bye byes!'\n",
    "\n",
    "\n",
    "def inject():\n",
    "    urls = [\n",
    "        'http://www.python.org', 'http://www.yahoo.com'\n",
    "        'http://www.scala.org', 'http://www.google.com'\n",
    "        # etc.. \n",
    "    ]\n",
    "    queue = Queue.Queue()\n",
    "    worker_threads = build_worker_pool(queue, 4)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Add the urls to process\n",
    "    for url in urls: \n",
    "        queue.put(url)  \n",
    "    # Add the poison pillv\n",
    "    for worker in worker_threads:\n",
    "        queue.put('quit')\n",
    "    for worker in worker_threads:\n",
    "        worker.join()\n",
    "\n",
    "    print 'Done! Time taken: {}'.format(time.time() - start_time)\n",
    "\n",
    "def build_worker_pool(queue, size):\n",
    "    workers = []\n",
    "    for _ in range(size):\n",
    "        worker = Fetcher(queue)\n",
    "        worker.start() \n",
    "        workers.append(worker)\n",
    "    return workers\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![map](https://awsblogstore.s3.amazonaws.com/main/images/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2 \n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "urls = [\n",
    "  'http://www.python.org', \n",
    "  'http://www.python.org/about/',\n",
    "  'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html',\n",
    "  'http://www.python.org/doc/',\n",
    "  'http://www.python.org/download/',\n",
    "  'http://www.python.org/getit/',\n",
    "  'http://www.python.org/community/',\n",
    "  'https://wiki.python.org/moin/',\n",
    "  'http://planet.python.org/',\n",
    "  'https://wiki.python.org/moin/LocalUserGroups',\n",
    "  'http://www.python.org/psf/',\n",
    "  'http://docs.python.org/devguide/',\n",
    "  'http://www.python.org/community/awards/'\n",
    "  # etc.. \n",
    "  ]\n",
    "\n",
    "# Make the Pool of workers\n",
    "pool = ThreadPool(4) \n",
    "# Open the urls in their own threads\n",
    "# and return the results\n",
    "results = pool.map(urllib2.urlopen, urls)\n",
    "#close the pool and wait for the work to finish \n",
    "pool.close() \n",
    "pool.join() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on true story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_pixel_k(config, vk_id):\n",
    "    url = \"http://%s/k?vk_id=%s\" % (config[\"PIXEL_K_HOST\"], vk_id)\n",
    "    response = requests.get(url, headers={'referer': \"http://vk.com\"}, timeout=5)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except Exception, e:\n",
    "        logging.error(\"pixel_k request error: %s\" % e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def refresh_main(config):\n",
    "    logging.info(\"VK profiles refresher started\")\n",
    "    num_profiles_processed = num_profiles_refreshed = 0\n",
    "    snap_dirs = filter(os.path.isdir, glob.glob(config[\"SNAPSHOTS_DIR_PATTERN\"]))\n",
    "    logging.info(\"%s snap dir(s) specified\" % len(snap_dirs))\n",
    "    sleep = 1.0 / 170\n",
    "    for snap_dir in snap_dirs:\n",
    "        logging.info(\"Looking for snaps in %s\" % snap_dir)\n",
    "        for profile in iter_vk_profiles(snap_dir, config[\"VK_PROFILES_SPACE\"]):\n",
    "            num_profiles_processed += 1\n",
    "            if is_stall(config, profile) or (profile.is_incomplete() and not profile.refreshed_recently()):\n",
    "                num_profiles_refreshed += 1\n",
    "                query_pixel_k(config, profile.vk_id)\n",
    "                time.sleep(sleep)\n",
    "    logging.info(\"%s profiles processed, %s refreshed\" % (num_profiles_processed, num_profiles_refreshed))\n",
    "    if not num_profiles_processed:\n",
    "        logging.error(\"No profiles processed during run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "def refresh_main(config):\n",
    "    logging.info(\"VK profiles refresher started\")\n",
    "    nprocessed = nrefreshed = 0\n",
    "    snap_dirs = filter(os.path.isdir, glob.glob(config[\"SNAPSHOTS_DIR_PATTERN\"]))\n",
    "    logging.info(\"%s snap dir(s) specified\" % len(snap_dirs))\n",
    "    for snap_dir in snap_dirs:\n",
    "        logging.info(\"Looking for snaps in %s\" % snap_dir)\n",
    "        pool = ThreadPool(config[\"NUM_WORKERS\"])\n",
    "        args_gen = ((config, t) for t in iter_snapshot(snap_dir, config[\"VK_PROFILES_SPACE\"]))\n",
    "        logging.info(\"Starting pool of %d threads working on snapshot\" % (config[\"NUM_WORKERS\"],))\n",
    "        for refreshed in pool.imap_unordered(update_vk_profile, args_gen, chunksize=1000):\n",
    "            nprocessed += 1\n",
    "            nrefreshed += refreshed\n",
    "            if not nrefreshed % 1000:\n",
    "                logging.info(\"Processed: %s. Refreshed: %s\" % (nprocessed, nrefreshed))\n",
    "    logging.info(\"%s profiles processed, %s refreshed\" % (nprocessed, nrefreshed))\n",
    "    if not nprocessed:\n",
    "        logging.error(\"No profiles processed during run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Queue\n",
    "import threading\n",
    "\n",
    "_sentinel = object()\n",
    "\n",
    "\n",
    "class VKProfileUpdater(threading.Thread):\n",
    "    def __init__(self, queue, config):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.daemon = True\n",
    "        self.queue = queue\n",
    "        self.config = config\n",
    "        self.refreshed = self.processed = 0\n",
    "        self.tnt_vk_profiles2 = self.memc_uid = None\n",
    "        ignore_api_errors = config[\"VK\"].get(\"ignore_errors\", True)\n",
    "        self.vk_api = VKAPI(config[\"VK\"][\"token\"], config[\"VK\"][\"salt\"], ignore_errors=ignore_api_errors)\n",
    "\n",
    "    # ...\n",
    "    # skipped code\n",
    "    # ...\n",
    "\n",
    "    def work(self, tuples):\n",
    "        profiles = (VKProfile.from_tuple(t) for t in tuples if t)\n",
    "        results = self.update_profiles(p for p in profiles if p and (p.needs_update() or self.config[\"FORCE\"]))\n",
    "        for updated in results:\n",
    "            self.processed += 1\n",
    "            self.refreshed += bool(updated)\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self._set_connections()\n",
    "        except Exception, e:\n",
    "            logging.exception(\"Cannot set requried connections: %s\" % e)\n",
    "            return\n",
    "        while True:\n",
    "            j = self.queue.get(block=True, timeout=None)\n",
    "            if j == _sentinel:\n",
    "                self.queue.task_done()\n",
    "                break\n",
    "            try:\n",
    "                self.work(j)\n",
    "            except Exception as e:\n",
    "                logging.exception(\"%s\" % repr(e))\n",
    "            self.queue.task_done()\n",
    "\n",
    "\n",
    "def refresh_main(config):\n",
    "    logging.info(\"VK profiles refresher started\")\n",
    "    read = nprocessed = nrefreshed = 0\n",
    "    snap_dirs = filter(os.path.isdir, glob.glob(config[\"SNAPSHOTS_DIR_PATTERN\"]))\n",
    "    logging.info(\"%s snap dir(s) specified\" % len(snap_dirs))\n",
    "    tuples_queue = Queue.Queue(maxsize=config[\"NUM_WORKERS\"])\n",
    "    logging.info(\"Starting pool of %d threads working on snapshot\" % (config[\"NUM_WORKERS\"],))\n",
    "    workers = []\n",
    "    for _ in range(config[\"NUM_WORKERS\"]):\n",
    "        worker = VKProfileUpdater(tuples_queue, config)\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    for snap_dir in snap_dirs:\n",
    "        logging.info(\"Looking for snaps in %s\" % snap_dir)\n",
    "        it = iter_snapshot(snap_dir, config[\"VK_PROFILES_SPACE\"])\n",
    "        chunk = list(itertools.islice(it, config[\"CHUNK_SIZE\"]))\n",
    "        while all(w.is_alive() for w in workers):\n",
    "            if not chunk:\n",
    "                break\n",
    "            try:\n",
    "                tuples_queue.put(chunk, block=False, timeout=60)\n",
    "            except Queue.Full:\n",
    "                continue\n",
    "            read += len(chunk)\n",
    "            logging.info(\"Read: %s. In queue: %s * %s\" % (read, tuples_queue.qsize(), config[\"CHUNK_SIZE\"]))\n",
    "            chunk = list(itertools.islice(it, config[\"CHUNK_SIZE\"]))\n",
    "        else:\n",
    "            logging.error(\"Not all workers alive. Exiting\")\n",
    "            return\n",
    "\n",
    "    for _ in workers:\n",
    "        tuples_queue.put(_sentinel)\n",
    "    logging.info(\"Waiting for queue to empty\")\n",
    "    while tuples_queue.qsize():\n",
    "        logging.info(\"Queue size: %s * %s\" % (tuples_queue.qsize(), config[\"CHUNK_SIZE\"]))\n",
    "        time.sleep(10)\n",
    "    for w in workers:\n",
    "        w.join()\n",
    "\n",
    "    nrefreshed = sum(w.refreshed for w in workers)\n",
    "    nprocessed = sum(w.processed for w in workers)\n",
    "    logging.info(\"%s profiles processed, %s refreshed\" % (nprocessed, nrefreshed))\n",
    "    if not nprocessed:\n",
    "        logging.error(\"No profiles processed during run\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite an overhead to using a Queue, due to the pickling and synchronization. If your task has a long completion time (at least a sizable fraction of a second) with a small amount of communication, then a Queue approach might be the right answer. You will have to verify whether the communication cost makes this approach useful enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be don't do that at home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TarantoolThread(object):\n",
    "    def __init__(self, tid):\n",
    "        self.id = tid\n",
    "\n",
    "    def execute(self, queue):\n",
    "        mappings = {\n",
    "            \"vk_id\": DeviceVKMappingTarantool,\n",
    "            \"ok_id\": DeviceOKMappingTarantool,\n",
    "            \"email\": DeviceEmailMappingTarantool,\n",
    "        }\n",
    "        records = 0\n",
    "        while True:\n",
    "            try:\n",
    "                task = queue.get(timeout=0.1)\n",
    "            except Queue.Empty:\n",
    "                logging.info(\"[%s] Records inserted: %s\" % (self.id, records))\n",
    "                return\n",
    "            tnt_pool, user_id_type, device_id_type, tuple_data = task\n",
    "            try:\n",
    "                mapping = tnt_pool.get(timeout=0.01)\n",
    "            except Queue.Empty:\n",
    "                mapping = mappings[user_id_type](config, device_id_type)\n",
    "            ok = mapping.insert(tuple_data)\n",
    "            if ok:\n",
    "                records += 1\n",
    "            tnt_pool.put(mapping)\n",
    "\n",
    "\n",
    "def handle_binlog(config, binlog_path):\n",
    "    pools = collections.defaultdict(Queue.Queue)\n",
    "    queue = Queue.Queue()\n",
    "    workers = []\n",
    "    fn = binlog_path.split(\"/\")[-1]\n",
    "    for i in range(config[\"THREADS_PER_WORKER\"]):\n",
    "        tnt_thread = TarantoolThread(\"%s:%s\" % (fn, i))\n",
    "        thread = threading.Thread(target=tnt_thread.execute, args=(queue, ))\n",
    "        workers.append((tnt_thread, thread))\n",
    "\n",
    "    for (tnt_thread, thread) in workers:\n",
    "        thread.start()\n",
    "\n",
    "    logging.info(\"Processing %s\" % binlog_path)\n",
    "    s = time.time()\n",
    "    for _, msg in lib.protoread(binlog_path):\n",
    "        for user_id_type in config[\"USER_ID_TYPES\"]:\n",
    "            user_ids = msg.get(user_id_type, [])\n",
    "            if not isinstance(user_ids, list):\n",
    "                user_ids = [user_ids]\n",
    "            for device_id_type in config[\"DEVICE_ID_TYPES\"]:\n",
    "                device_id = msg.get(device_id_type)\n",
    "                if not device_id:\n",
    "                    continue\n",
    "                for user_id in user_ids:\n",
    "                    td = {user_id_type: user_id, \"device_id\": device_id, \"last_update\": msg[\"timestamp\"]}\n",
    "                    queue.put((pools[(user_id_type, device_id_type)], user_id_type, device_id_type, td))\n",
    "\n",
    "    for (tnt_thread, thread) in workers:\n",
    "        thread.join()\n",
    "    logging.info(\"%s processed in %s sec\" % (binlog_path, time.time() - s))\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    master = lib.Master(config[\"WORKERS\"])\n",
    "    binlogs_dir = config[\"BINLOGS_DIR\"]\n",
    "\n",
    "    while True:\n",
    "        master.fetch()\n",
    "        got_work = False\n",
    "        if config[\"WORKERS\"] - len(master.jobs) > 0:\n",
    "            binlogs = [Binlog(binlogs_dir, fn) for fn in os.listdir(binlogs_dir) if BINLOG_RE.match(fn)]\n",
    "            binlogs = sorted(binlogs, key=lambda b: b.datetime)\n",
    "            logging.info(\"Found %s binlogs at %s\" % (len(binlogs), binlogs_dir))\n",
    "            for b in binlogs:\n",
    "                got_work = True\n",
    "                tmp_rename_path = os.path.join(b.dir, \".\" + b.name)\n",
    "                finish_path = os.path.join(b.dir, \"_\" + b.name)\n",
    "                d = {\"path\": tmp_rename_path, \"finish_path\": finish_path}\n",
    "                os.rename(b.path, tmp_rename_path)\n",
    "                master.job(handle_binlog, config, tmp_rename_path, multijob=d)\n",
    "\n",
    "        while True:\n",
    "            if not master.jobs or \"result\" not in master.jobs[0]:\n",
    "                break\n",
    "\n",
    "            finished = master.jobs.pop(0)\n",
    "            logging.info(\"Finishing %s\" % finished[\"multijob\"][\"path\"])\n",
    "            os.rename(finished[\"multijob\"][\"path\"], finished[\"multijob\"][\"finish_path\"])\n",
    "\n",
    "        if not got_work:\n",
    "            time.sleep(config[\"LOG_WAIT_TIMEOUT\"])\n",
    "\n",
    "# ...\n",
    "# skipped code\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![proc](http://sebastianraschka.com/images/blog/2014/multiprocessing_intro/multiprocessing_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import_mock.py'''\n",
    "to_mock = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_mock\n",
    "from multiprocessing import Process\n",
    "\n",
    "class A(object):\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "        self.b = 2\n",
    "        self.c = 3\n",
    "\n",
    "    def __getstate__(self):\n",
    "        print '__getstate__'\n",
    "        return { 'a': self.a, 'b': self.b, 'c': 0}\n",
    "\n",
    "def func():\n",
    "    import_mock.to_mock = 1\n",
    "    a = A()\n",
    "    return a\n",
    "\n",
    "def func1(a):\n",
    "    print a.a, a.b, a.c\n",
    "    print import_mock.to_mock\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = func()\n",
    "    p = Process(target=func1, args=(a,))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "__getstate__\n",
    "1 2 0\n",
    "None\n",
    "\n",
    "# Linux\n",
    "1 2 3\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import_mock:\n",
    "\n",
    "* On all platforms, the main process calls `func()`, which sets import_mock.to_mock to 1.\n",
    "* On Unix-y platforms, that's what all new processes see: the `fork()` occurs after that, so 1 is the state all new processes inherit.\n",
    "* On Windows, all new processes run the entire module \"from scratch\". So they each import their own, brand new version of import_mock. Only the main process calls `func()`, so only the main process sees to_mock change to 1. All other processes see the fresh None state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.a, a.b, a.c:\n",
    "* Because of copy-on-write `fork()` semantics, it wasn't necessary to pickle `Process()` arguments on Unix-y systems, and so the implementation never did.\n",
    "* However, without `fork()` it is necessary to pickle them on Windows - and so the implementation does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new processes are launched differently depending on the version of python and the platform on which the code is running e.g.:\n",
    "* Windows uses `spawn` to create the new process.\n",
    "* With unix systems and version earlier than 3.3, the processes are created using a `fork`. Note that this method does not respect the POSIX usage of fork and thus leads to unexpected behaviors, especially when interacting with other multiprocessing libraries.\n",
    "* With unix system and version 3.4+, you can choose to start the new processes with either `fork`, `forkserver` or `spawn` using `multiprocessing.set_start_method` at the beginning of your program. `forkserver` and `spawn` methods are slower than forking but avoid some unexpected behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `fork`, a new process will be launched with the exact same state for all the current mutex but only the `MainThread` will be launched. This is unsafe as it could lead to race conditions e.g.:\n",
    "If you use a `Lock` in `MainThread` and pass it to another thread which is supposed to lock it at some point. If the fork occurs simultaneously, the new process will start with a locked lock which will never be released as the second thread does not exist in this new process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pipe__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipe](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/multiprocessing-python-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.48905944824e-05\n",
      "0.00908398628235\n",
      "0.0135841369629\n",
      "0.0171611309052\n",
      "0.0200011730194\n",
      "0.0242128372192\n",
      "0.0264618396759\n",
      "0.0316610336304\n",
      "0.0344798564911\n",
      "0.0388178825378\n",
      "0.0431010723114\n",
      "0.046037197113\n",
      "0.0495488643646\n",
      "0.0528440475464\n",
      "0.056972026825\n",
      "0.0599360466003\n",
      "0.0630559921265\n",
      "0.0657181739807\n",
      "0.0680620670319\n",
      "0.0709049701691\n",
      "0.0740120410919\n",
      "0.076623916626\n",
      "0.079066991806\n",
      "0.0820689201355\n",
      "0.0853588581085\n",
      "0.0884990692139\n",
      "0.0911569595337\n",
      "0.0939629077911\n",
      "0.0967259407043\n",
      "0.0998439788818\n",
      "0.102437019348\n",
      "0.105307817459\n",
      "0.108021020889\n",
      "0.110898017883\n",
      "0.113418102264\n",
      "0.117795944214\n",
      "0.124101877213\n",
      "0.129719018936\n",
      "0.132791996002\n",
      "0.136101961136\n",
      "0.143686056137\n",
      "0.147948026657\n",
      "0.150674104691\n",
      "0.153499126434\n",
      "0.157027959824\n",
      "0.15985584259\n",
      "0.16263794899\n",
      "0.165544033051\n",
      "0.168145895004\n",
      "0.172059059143\n",
      "0.175159931183\n",
      "0.178770065308\n",
      "0.182589054108\n",
      "0.185470104218\n",
      "0.188155889511\n",
      "0.191532850266\n",
      "0 0.00215196609497\n",
      "0 0.00239300727844\n",
      "0 0.00244498252869\n",
      "1 0.0022919178009\n",
      "1 0.000534057617188\n",
      "1 0.000603914260864\n",
      "1 0.000579118728638\n",
      "1 0.000453948974609\n",
      "1 0.000515937805176\n",
      "1 0.000536918640137\n",
      "1 0.000877141952515\n",
      "1 0.000926971435547\n",
      "1 0.000964879989624\n",
      "1 0.000916004180908\n",
      "1 0.00100517272949\n",
      "1 0.00106406211853\n",
      "1 0.00113105773926\n",
      "1 0.00116109848022\n",
      "1 0.00124597549438\n",
      "1 0.0012378692627\n",
      "1 0.00115513801575\n",
      "1 0.00105595588684\n",
      "1 0.00117611885071\n",
      "1 0.00122904777527\n",
      "1 0.00141000747681\n",
      "1 0.00133299827576\n",
      "1 0.00142407417297\n",
      "1 0.00147008895874\n",
      "1 0.00147700309753\n",
      "1 0.00136590003967\n",
      "0.194323062897\n",
      "0.197507143021\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from multiprocessing import Process, Queue, Pipe\n",
    "import sys\n",
    "import time #For the timestamp and sleep function\n",
    "if sys.platform == \"win32\":\n",
    "    # On Windows, the best timer is time.clock()\n",
    "    default_timer = time.clock\n",
    "else:\n",
    "    # On most other platforms, the best timer is time.time()\n",
    "    default_timer = time.time\n",
    "\n",
    "def pa(child_conn, parent_q):\n",
    "    keep_running = True\n",
    "    while keep_running:\n",
    "        r = random.randint(0,10)\n",
    "        if r == 10:\n",
    "            parent_q.put([0, default_timer()])\n",
    "        if child_conn.poll():\n",
    "            msg = child_conn.recv()\n",
    "            this_time = default_timer()\n",
    "            if msg == 'quit':\n",
    "                keep_running = False\n",
    "            else:\n",
    "                print this_time - msg\n",
    "\n",
    "def pb(child_conn, parent_q):\n",
    "    keep_running = True\n",
    "    while keep_running:\n",
    "        r = random.randint(0,10)\n",
    "        if r == 10:\n",
    "            parent_q.put([1, default_timer()])\n",
    "        if child_conn.poll():\n",
    "            msg = child_conn.recv()\n",
    "            this_time = default_timer()\n",
    "            if msg == 'quit':\n",
    "                keep_running = False\n",
    "            else:  \n",
    "                print this_time - msg\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn0, child_conn0 = Pipe()\n",
    "    parent_conn1, child_conn1 = Pipe()\n",
    "    parent_q = Queue()\n",
    "    p0 = Process(target=pa, args=(child_conn0, parent_q))\n",
    "    p0.start()\n",
    "    p1 = Process(target=pb, args=(child_conn1, parent_q))\n",
    "    p1.start()\n",
    "    keep_running = True\n",
    "    while keep_running:\n",
    "        ans = parent_q.get()#Blocking get\n",
    "        this_time = default_timer()\n",
    "        print ans[0], this_time - ans[1]\n",
    "        r = random.randint(0,20)\n",
    "        if r == 10:\n",
    "            parent_conn0.send('quit')\n",
    "            parent_conn1.send('quit')\n",
    "            keep_running = False\n",
    "        else:\n",
    "            parent_conn0.send(default_timer())\n",
    "            parent_conn0.send(default_timer())\n",
    "    p0.join()\n",
    "    p1.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Queue__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![queue](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/multiprocessing-python-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RhPcf', '0ft7T', 'Q8hfh', 'Fmrw1']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "import string\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "# Define an output queue\n",
    "output = mp.Queue()\n",
    "\n",
    "# define a example function\n",
    "def rand_string(length, output):\n",
    "    \"\"\" Generates a random string of numbers, lower- and uppercase chars. \"\"\"\n",
    "    rand_str = ''.join(random.choice(\n",
    "                        string.ascii_lowercase\n",
    "                        + string.ascii_uppercase\n",
    "                        + string.digits)\n",
    "                   for i in range(length))\n",
    "    output.put(rand_str)\n",
    "\n",
    "# Setup a list of processes that we want to run\n",
    "processes = [mp.Process(target=rand_string, args=(5, output)) for x in range(4)]\n",
    "\n",
    "# Run processes\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "# Exit the completed processes\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# Get process results from the output queue\n",
    "results = [output.get() for p in processes]\n",
    "\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that each process we create consumes some RAM from the system. You can expect a forked process using the standard libraries to take on the order of 10–20MB of RAM; if you’re using many libraries and lots of data, then you might expect each forked copy to take hundreds of megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "from multiprocessing import Pool\n",
    "from time import sleep\n",
    " \n",
    "# The size of the cache created when calling range(CACHE). To ensure that your\n",
    "# processes is visible on your system, you will need to adjust this value.\n",
    "# Start with the CACHE value below and keep adding 0s until anywhere between\n",
    "# 5-10% of your memory is consumed. You may also want to reduce the PAUSE\n",
    "# value below to make this tuning easier. Note that IT IS VERY EASY TO RUN\n",
    "# YOUR SYSTEM OUT OF MEMORY THIS WAY, so save everything before attempting.\n",
    "CACHE = 10000\n",
    " \n",
    "# The number of subprocesses forked.\n",
    "PROCESSES = 5\n",
    " \n",
    "# The number of seconds to pause in order to display the allocated memory in\n",
    "# the system monitor.\n",
    "PAUSE = 3\n",
    " \n",
    "def job(cache):\n",
    "    '''\n",
    "    An artificial job that sleeps for a second and then reports memory usage.\n",
    "    '''\n",
    "    # Read all of the data in the cache. Note that in order to see the effect\n",
    "    # the copy on write problem, we need to actually read the cache. By reading\n",
    "    # the cache, we create a new reference to the memory being tracked and\n",
    "    # therefore copy the memory although all we're doing is reading.\n",
    "    for item in cache:\n",
    "        pass\n",
    " \n",
    "    # Make sure the memory that the allocated memory is visible in the system\n",
    "    # monitor.\n",
    "    sleep(PAUSE)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Entry point.\n",
    "    '''\n",
    "    # Create the cache. Note that allocating a large amount of memory takes\n",
    "    # time. After the memory is allocated, we want to make sure the new memory\n",
    "    # level is visible in the system monitor, so we pause.\n",
    "    cache = range(CACHE)\n",
    "    sleep(PAUSE)\n",
    " \n",
    "    # Run the jobs.\n",
    "    pool = Pool(PROCESSES)\n",
    "    pool.map(job, [cache]*PROCESSES)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cow](https://llvllatrix.files.wordpress.com/2016/02/benchmark1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: pool methods all use a queue.Queue to pass tasks to the worker processes. Everything that goes through the queue.Queue must be pickable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shared](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/multiprocessing-python-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import timeit\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import create_range\n",
    "\n",
    "\n",
    "SERIAL_CHECK_CUTOFF = 21\n",
    "CHECK_EVERY = 1000\n",
    "FLAG_CLEAR = b'0'\n",
    "FLAG_SET = b'1'\n",
    "print \"CHECK_EVERY\", CHECK_EVERY\n",
    "\n",
    "\n",
    "def check_prime_in_range(args):\n",
    "    (n, (from_i, to_i), value) = args\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    check_every = CHECK_EVERY\n",
    "    for i in xrange(from_i, int(to_i), 2):\n",
    "        check_every -= 1\n",
    "        if not check_every:\n",
    "            if value.value == FLAG_SET:\n",
    "                return False\n",
    "            check_every = CHECK_EVERY\n",
    "\n",
    "        if n % i == 0:\n",
    "            value.value = FLAG_SET\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_prime(n, pool, nbr_processes, value):\n",
    "    # cheaply check high probability set of possible factors\n",
    "    from_i = 3\n",
    "    to_i = SERIAL_CHECK_CUTOFF\n",
    "    value.value = FLAG_CLEAR\n",
    "    if not check_prime_in_range((n, (from_i, to_i), value)):\n",
    "        return False\n",
    "    value.value = FLAG_CLEAR\n",
    "\n",
    "    from_i = to_i\n",
    "    to_i = int(math.sqrt(n)) + 1\n",
    "\n",
    "    ranges_to_check = create_range.create(from_i, to_i, nbr_processes)\n",
    "    ranges_to_check = zip(\n",
    "        len(ranges_to_check) * [n],\n",
    "        ranges_to_check,\n",
    "        len(ranges_to_check) * [value])\n",
    "    assert len(ranges_to_check) == nbr_processes\n",
    "    results = pool.map(check_prime_in_range, ranges_to_check)\n",
    "    if False in results:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NBR_PROCESSES = 4\n",
    "    value = multiprocessing.Value(b'c', FLAG_CLEAR)  # 1 byte character\n",
    "    pool = Pool(processes=NBR_PROCESSES)\n",
    "    print \"Testing with {} processes\".format(NBR_PROCESSES)\n",
    "    labeled_numbers = [\n",
    "        (\"trivial non-prime\", 112272535095295),\n",
    "        (\"expensive non-prime18_1\", 100109100129100369),\n",
    "        (\"expensive non-prime18_2\", 100109100129101027),\n",
    "        (\"prime18_1\", 100109100129100151),\n",
    "        (\"prime18_2\", 100109100129162907),\n",
    "    ]\n",
    "    for label, nbr in labeled_numbers:\n",
    "        time_costs = timeit.repeat(\n",
    "            stmt=\"check_prime({}, pool, {}, value)\".format(nbr, NBR_PROCESSES),\n",
    "            repeat=20,\n",
    "            number=1,\n",
    "            setup=\"from __main__ import pool, check_prime, value\")\n",
    "        print \"{:19} ({}) {: 3.6f}s\".format(label, nbr, min(time_costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Array__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(in process p1): [1, 4, 9, 16]\n",
      "Sum of squares(in process p1): 30\n",
      "Result(in main program): [1, 4, 9, 16]\n",
      "Sum of squares(in main program): 30\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    " \n",
    "def square_list(mylist, result, square_sum):\n",
    "    \"\"\"\n",
    "    function to square a given list\n",
    "    \"\"\"\n",
    "    # append squares of mylist to result array\n",
    "    for idx, num in enumerate(mylist):\n",
    "        result[idx] = num * num\n",
    " \n",
    "    # square_sum value\n",
    "    square_sum.value = sum(result)\n",
    " \n",
    "    # print result Array\n",
    "    print(\"Result(in process p1): {}\".format(result[:]))\n",
    " \n",
    "    # print square_sum Value\n",
    "    print(\"Sum of squares(in process p1): {}\".format(square_sum.value))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # input list\n",
    "    mylist = [1,2,3,4]\n",
    " \n",
    "    # creating Array of int data type with space for 4 integers\n",
    "    result = multiprocessing.Array('i', 4)\n",
    " \n",
    "    # creating Value of int data type\n",
    "    square_sum = multiprocessing.Value('i')\n",
    " \n",
    "    # creating new process\n",
    "    p1 = multiprocessing.Process(target=square_list, args=(mylist, result, square_sum))\n",
    " \n",
    "    # starting process\n",
    "    p1.start()\n",
    " \n",
    "    # wait until process is finished\n",
    "    p1.join()\n",
    " \n",
    "    # print result array\n",
    "    print(\"Result(in main program): {}\".format(result[:]))\n",
    " \n",
    "    # print square_sum Value\n",
    "    print(\"Sum of squares(in main program): {}\".format(square_sum.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mmap__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Check primality by splitting the list of factors with early prime check and mmap flag\"\"\"\n",
    "\n",
    "import math\n",
    "import timeit\n",
    "from multiprocessing import Pool\n",
    "import create_range\n",
    "import mmap\n",
    "\n",
    "\n",
    "SERIAL_CHECK_CUTOFF = 21\n",
    "CHECK_EVERY = 1000\n",
    "FLAG_CLEAR = b'0'\n",
    "FLAG_SET = b'1'\n",
    "print \"CHECK_EVERY\", CHECK_EVERY\n",
    "\n",
    "sh_mem = mmap.mmap(-1, 1)  # memory map 1 byte as a flag\n",
    "\n",
    "\n",
    "def check_prime_in_range(args):\n",
    "    (n, (from_i, to_i)) = args\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    for outer_counter in xrange(from_i, int(to_i), CHECK_EVERY):\n",
    "        upper_bound = min(int(to_i), outer_counter + CHECK_EVERY)\n",
    "        for i in xrange(outer_counter, upper_bound, 2):\n",
    "            if n % i == 0:\n",
    "                sh_mem.seek(0)\n",
    "                sh_mem.write_byte(FLAG_SET)\n",
    "                return False\n",
    "        sh_mem.seek(0)\n",
    "        flag = sh_mem.read_byte()\n",
    "        if flag == FLAG_SET:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_prime(n, pool, nbr_processes):\n",
    "    # cheaply check high probability set of possible factors\n",
    "    from_i = 3\n",
    "    to_i = SERIAL_CHECK_CUTOFF\n",
    "    sh_mem.seek(0)\n",
    "    sh_mem.write_byte(FLAG_CLEAR)\n",
    "    if not check_prime_in_range((n, (from_i, to_i))):\n",
    "        return False\n",
    "    sh_mem.seek(0)\n",
    "    sh_mem.write_byte(FLAG_CLEAR)\n",
    "\n",
    "    from_i = to_i\n",
    "    to_i = int(math.sqrt(n)) + 1\n",
    "\n",
    "    ranges_to_check = create_range.create(from_i, to_i, nbr_processes)\n",
    "    ranges_to_check = zip(len(ranges_to_check) * [n], ranges_to_check)\n",
    "    assert len(ranges_to_check) == nbr_processes\n",
    "    results = pool.map(check_prime_in_range, ranges_to_check)\n",
    "    if False in results:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![manager](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/multiprocessing-python-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New record added!\n",
      "\n",
      "Name: Sam\n",
      "Score: 10\n",
      "\n",
      "Name: Adam\n",
      "Score: 9\n",
      "\n",
      "Name: Kevin\n",
      "Score: 9\n",
      "\n",
      "Name: Jeff\n",
      "Score: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    " \n",
    "def print_records(records):\n",
    "    \"\"\"\n",
    "    function to print record(tuples) in records(list)\n",
    "    \"\"\"\n",
    "    for record in records:\n",
    "        print(\"Name: {0}\\nScore: {1}\\n\".format(record[0], record[1]))\n",
    "\n",
    "def insert_record(record, records):\n",
    "    \"\"\"\n",
    "    function to add a new record to records(list)\n",
    "    \"\"\"\n",
    "    records.append(record)\n",
    "    print(\"New record added!\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        # creating a list in server process memory\n",
    "        records = manager.list([('Sam', 10), ('Adam', 9), ('Kevin',9)])\n",
    "        # new record to be inserted in records\n",
    "        new_record = ('Jeff', 8)\n",
    " \n",
    "        # creating new processes\n",
    "        p1 = multiprocessing.Process(target=insert_record, args=(new_record, records))\n",
    "        p2 = multiprocessing.Process(target=print_records, args=(records,))\n",
    " \n",
    "        # running process p1 to insert new record\n",
    "        p1.start()\n",
    "        p1.join()\n",
    " \n",
    "        # running process p2 to print records\n",
    "        p2.start()\n",
    "        p2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.connection import Listener\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def handle_client(c):\n",
    "    while True:\n",
    "        msg = c.recv()\n",
    "        c.send(msg)\n",
    "\n",
    "\n",
    "def echo_server(address, authkey):\n",
    "    server_c = Listener(address, authkey=authkey)\n",
    "    while True:\n",
    "        client_c = server_c.accept()\n",
    "        t = Thread(target=handle_client, args=(client_c,))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "\n",
    "if __name__ == '_ _main_ _':\n",
    "    echo_server((\"\",16000), \"peekaboo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from multiprocessing.connection import Client\n",
    ">>> c = Client((\"localhost\",16000), authkey=\"peekaboo\")\n",
    ">>> c.send(\"Hello\")\n",
    ">>> c.recv()\n",
    "\"Hello\"\n",
    ">>> c.send([1,2,3,4])\n",
    ">>> c.recv()\n",
    "[1, 2, 3, 4]\n",
    ">>> c.send({\"name\":\"Dave\",\"email\":\"dave@dabeaz.com\"})\n",
    ">>> c.recv()\n",
    "{\"name\": \"Dave\", \"email\": \"dave@dabeaz.com\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpcserver.py\n",
    "from multiprocessing.connection import Listener, Client\n",
    "from threading import Thread\n",
    "\n",
    "class RPCServer(object):\n",
    "    def __init__(self, address, authkey):\n",
    "        self._functions = {}\n",
    "        self._server_c = Listener(address, authkey=authkey)\n",
    "\n",
    "    def register_function(self, func):\n",
    "        self._functions[func.__name__] = func\n",
    "\n",
    "    def serve_forever(self):\n",
    "        while True:\n",
    "            client_c = self._server_c.accept()\n",
    "            t = Thread(target=self.handle_client, args=(client_c,))\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "\n",
    "    def handle_client(self, client_c):\n",
    "        while True:\n",
    "            func_name, args, kwargs = client_c.recv()\n",
    "            try:\n",
    "                r = self._functions[func_name](*args, **kwargs)\n",
    "                client_c.send(r)\n",
    "            except Exception as e:\n",
    "                client_c.send(e)\n",
    "\n",
    "\n",
    "class RPCProxy(object):\n",
    "    def __init__(self, address, authkey):\n",
    "        self._conn = Client(address, authkey=authkey)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        def do_rpc(*args, **kwargs):\n",
    "            self._conn.send((name, args, kwargs))\n",
    "            result = self._conn.recv()\n",
    "            if isinstance(result, Exception):\n",
    "                raise result\n",
    "            return result\n",
    "        return do_rpc\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    def add(x, y):\n",
    "        return x + y\n",
    "    def sub(x, y):\n",
    "        return x - y\n",
    "\n",
    "    # Create and run the server\n",
    "    serv = RPCServer((\"localhost\", 17000), authkey=\"peekaboo\")\n",
    "    serv.register_function(add)\n",
    "    serv.register_function(sub)\n",
    "    serv.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from rserver import RPCProxy\n",
    ">>> c = RPCProxy((\"localhost\", 17000), authkey=\"peekaboo\")\n",
    ">>> c.add(2, 3)\n",
    "5\n",
    ">>> c.sub(2, 3)\n",
    "-1\n",
    ">>> c.sub([1, 2], 4)\n",
    "Traceback (most recent call last):\n",
    " File \"\", line 1, in\n",
    " File \"rpcserver.py\", line 37, in do_rpc\n",
    " raise result\n",
    "TypeError: unsupported operand type(s) for -: ‘list’ and ‘int’\n",
    ">>> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://en.wikipedia.org/wiki/Amdahl%27s_law\n",
    "* http://www.dabeaz.com/GIL/gilvis/index.html\n",
    "* http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing/\n",
    "* http://effbot.org/zone/thread-synchronization.htm\n",
    "* https://www.laurentluce.com/posts/python-threads-synchronization-locks-rlocks-semaphores-conditions-events-and-queues/\n",
    "* https://www.ibm.com/developerworks/aix/library/au-spunix_sharedmemory/index.html\n",
    "* https://www.usenix.org/system/files/login/articles/login1210_beazley.pdf\n",
    "* http://www.linuxdevcenter.com/pub/a/linux/2007/05/24/semaphores-in-linux.html\n",
    "* https://software.intel.com/en-us/articles/implementing-scalable-atomic-locks-for-multi-core-intel-em64t-and-ia32-architectures\n",
    "* http://www.linuxdevcenter.com/pub/a/linux/2007/05/24/semaphores-in-linux.html\n",
    "* http://marcoguerri.github.io/jekyll/python/multiprocessing/2016/07/24/python-multiprocessing-internals.html\n",
    "* https://www.geeksforgeeks.org/multiprocessing-python-set-2/\n",
    "* https://eli.thegreenplace.net/2012/01/24/distributed-computing-in-python-with-multiprocessing\n",
    "* http://bhfsteve.blogspot.ru/2013/02/fun-with-pythons-multiprocessing-module.html\n",
    "* https://llvllatrix.wordpress.com/2016/02/19/python-vs-copy-on-write/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use threads for I/O-intensive jobs and processes for CPU-intensive\n",
    "* look for \"embarrassingly parallel\" tasks\n",
    "* any synchronization or IPC comes at costs\n",
    "* \"more workers\" doesn't always imply \"more speed\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
